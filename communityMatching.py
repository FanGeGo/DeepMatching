#!/usr/bin/python
import numpy as np
import community as cmty
import os
import time as ti
import networkx as nx
from scipy.io import loadmat
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import time
from graph_matching import * 
import sys
import cnm
from sklearn.metrics.pairwise import *
import snap
import deepMatching as dm
from itertools import izip
from credibility import *
from refinement import *

def transform_snap_to_networkx(SG):
	'''
	Transforming the graph of snap specified by the @SG to the graph of networkx
	Return the graph of networkx as the result of this function

	Parameters
	----------

	SG : snap graph

	Returns
	-------

	G_networkx : networkx graph
		networkx graph generated by transforming the Snap graph 

	'''
	G_networkx = nx.Graph()
	#add the nodes from the SG
	for node in SG.Nodes():
		G_networkx.add_node(node.GetId())
	#add edges to the G_networkx
	for i in G_networkx.nodes():
		for j in G_networkx.nodes():
			if SG.IsEdge(i,j):
				G_networkx.add_edge(i,j)
	return G_networkx	
def transform_networkx_to_snap(G):
	'''
	Transforming the graph of networkx specified by the @G to the graph of snap 
	Return the graph of snap as the result of this function

	Parameters
	----------

	G : netoworkx
		the source networkx graph
	
	Returns
	-------

	TG : snap graph
		snap graph generated by transforming to the networkx graph

	'''
	TG = snap.TUNGraph.New()
	print "start transform the graph format from the networkx to snap ......"
	start_time = time.time()
	#add the nodes from the G
	print len(G.nodes())
	for node in G.nodes():
		TG.AddNode(node)
	#add edges to the TG
	for edge in G.edges():
		TG.AddEdge(edge[0],edge[1])

	print "transform finished,the Graph nodes: %d\t edges: %d" % (TG.GetNodes(),TG.GetEdges())
	running_time = time.time() - start_time
	print "sample finished,running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
	return TG
	

def bi_sample_graph(nx_G,sample_rate = 0.8):
	'''
		Returns two ntworkx graphes generated by applyin edge sampling process to the fix networkx graph 'nx_G'

	Paratemers
	----------

	nx_G : networkx graph
		the original graph
	
	sample_rate : float, nonegative and  less than 1
		the edge sampling process apply to original graph to generate two networkx graphes by using the sampling rate  

	Returns
	-------
	G1,G2 : networkx graph
		the subgraph after sampling

	'''
	G1 = sample_graph(nx_G, sample_rate)
	G2 = sample_graph(nx_G, sample_rate)
	return G1,G2
	
def community_detect_graph(G1,G2,detect_method = None):
	'''
	detect community and generate graph with community as nodes and the number edges between communities as weigth between new nodes  

	Parameters
	----------

	G1,G2 : networkx graph
		original graphes to match 
	
	detect_method : pionter of function
		specified the name of detection method
	
	Returns
	-------

	SG1,SG2 : Snap graph
		Snap graph generated with original networkx graph 'G' 

	SG1_ret_list : List
		consists of the communities detected by using detection method in G1 and every community contains the nodes
	[[node1,node3,...],[node2,node5,...],...]	


	SG2_ret_list : List
		consists of the communities detected by using detection method in G2 and every community contains the nodes
	[[node1,node3,...],[node2,node5,...],...]	

	'''
	print "start sample ......"

	start_time = time.time()
	#transforms SG to networkx
	if detect_method == None :
		#detect with the cnm
		SG1 = transform_networkx_to_snap(G1)
		SG2 = transform_networkx_to_snap(G2)
		SG1_ret_list_cnm = cnm.community_cnm(SG1)
		SG2_ret_list_cnm = cnm.community_cnm(SG2)
		SG1 = G1
		SG2 = G2
		SG1_ret_list_spectral = graph_spectral.partition_super_cliques(SG1)
		SG2_ret_list_spectral = graph_spectral.partition_super_cliques(SG2)
		running_time = time.time() - start_time
		print "sample finished,running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
		return SG1_ret_list_cnm,SG2_ret_list_cnm,SG1_ret_list_spectral,SG2_ret_list_spectral
	if detect_method == cnm.community_cnm_with_limit:
		SG1 = transform_networkx_to_snap(G1)
		SG2 = transform_networkx_to_snap(G2)
		print "limit nodes of the community for G1 : %d" % (len(G1.nodes()) / 8)
		SG1_ret_list = detect_method(SG1,len(G1.nodes())/8)
		print "limit nodes of the community for G2 : %d" % (len(G2.nodes()) / 8)
		SG2_ret_list = detect_method(SG2,len(G2.nodes()) /8)
		print "SG1 community size: %d \t SG2 community size :%d " % (len(SG1_ret_list),len(SG2_ret_list))
		running_time = time.time() - start_time
		print "sample finished,running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
		return SG1,SG2,SG1_ret_list,SG2_ret_list
	if detect_method == cmty.best_partition:
		s1_partition = cmty.best_partition(G1)
		s2_partition = cmty.best_partition(G2)
		SG1_ret_list = []
		SG2_ret_list = []
		for com in set(s1_partition.values()):
			temp = [nodes for nodes in s1_partition.keys() if s1_partition[nodes] == com]
			SG1_ret_list.append(temp)
		for com in set(s2_partition.values()):
			temp = [nodes for nodes in s2_partition.keys() if s2_partition[nodes] == com]
			SG2_ret_list.append(temp)
		running_time = time.time() - start_time
		print "SG1 community size: %d \t SG2 community size :%d " % (len(SG1_ret_list),len(SG2_ret_list))
		print "sample finished,running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
		SG1 = transform_networkx_to_snap(G1)
		SG2 = transform_networkx_to_snap(G2)
		return SG1,SG2,SG1_ret_list,SG2_ret_list

	if detect_method == cnm.community_best_partition_with_limit:
		SG1_ret_list = cnm.community_best_partition_with_limit(G1,2000)
		SG2_ret_list = cnm.community_best_partition_with_limit(G2,2000)

		print "SG1 community size: %d \t SG2 community size :%d " % (len(SG1_ret_list),len(SG2_ret_list))
		running_time = time.time() - start_time
		print "sample finished,running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
		SG1 = transform_networkx_to_snap(G1)
		SG2 = transform_networkx_to_snap(G2)
		return SG1,SG2,SG1_ret_list,SG2_ret_list

	if detect_method == cnm.community_cnm:
		SG1 = transform_networkx_to_snap(G1)
		SG2 = transform_networkx_to_snap(G2)
	else:
		SG1 = G1
		SG2 = G2
	SG1_ret_list = detect_method(SG1)
	SG2_ret_list = detect_method(SG2)

	running_time = time.time() - start_time
	print "SG1 community size: %d \t SG2 community size :%d " % (len(SG1_ret_list),len(SG2_ret_list))
	print "sample finished,running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
	
	return SG1,SG2,SG1_ret_list,SG2_ret_list
	 
	
def transform_gml_to_networkx(gml_filename):
	'''
	load the graph from gml file to generate the networkx graph

	Parameters
	----------

	gml_filename : string
		gml file path
	
	Returns
	-------

	G : networkx
		the networkx graph generated from the gml file	

	'''
	sf = open(gml_filename)
	G = nx.Graph()
	
	line = sf.readline()
	while "edge" not in line:
		line = sf.readline()

	while line != '':
		line = sf.readline()
		if line == '':
			break
		line = sf.readline()
		if line == '':
			break
		line = line.split(' ')

		source = int(line[-1][:-1]) + 1
		line = sf.readline()
		if line == '':
			break
		line = line.split(' ')
		dest = int(line[-1][:-1]) + 1
		G.add_edge(source,dest)
		line = sf.readline()
		if line == '':
			break
		line = sf.readline()

	sf.close()
	print "load finished,the Graph nodes: %d\t edges: %d" % (len(G.nodes()),len(G.edges()))
	return G


def load_graph_from_file(filename,comments = '#',delimiter = ' '):
	'''
		load the graph from file to generate the networkx graph

	Parameters
	----------

	filename : string
		contain the nodes and edges of graph
	
	comments : char 
		the line begined with '#' is commentary
	
	delimiter : char
		separate and obstract the nodes or the edges in every vaild line  
	
	Returns
	-------
	
	G : networkx graph
		create the new graph with graph file

	'''
	print "start load graph from file: %s" % filename
	if ".gml" in filename:
		return transform_gml_to_networkx(filename)
	G = nx.Graph()
	start_time = time.time()
	sf = open(filename)
	line = sf.readline()
	while line != '':
		if comments not in line:
			break
		line = sf.readline()
	while line != '':
		line = line.split(delimiter)
		# plus 1 reprents that the index of nodes is start from 1 at least(For sanp graph ,the nodes' index must start from 1 at least)
		G.add_edge(int(line[0]) + 1,int(line[1][:-1]) + 1)
		#G.add_edge(int(line[0]),int(line[1][:-1]))
		line = sf.readline()
	running_time = time.time() - start_time
	#print "G nodes: %d \t edges: %d" % (len(G.nodes()),len(G.edges()))
	#print "running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
	print "load graph finish!! "
	sf.close()
	return G

def draw_networkx(G):
	'''
	draw the networkx graph

	Parameters
	----------

	G : networkx
		original graph
	Returns:
	--------

	No return
	'''
	pos = nx.spring_layout(G)
	nx.draw_networkx_nodes(G,pos);
	nx.draw_networkx(G,pos)
	plt.show()
	return


def draw_networkx_with_weight(G):
	'''
	draw the networkx graph

	Parameters
	----------

	G : networkx
		original graph
	Returns:
	--------

	No return
	'''

	pos = nx.spring_layout(G)
	nx.draw_networkx_nodes(G,pos);
	nx.draw_networkx_edges(G,pos);
	nx.draw_networkx_labels(G,pos);
	nx.draw_networkx_edge_labels(G,pos,edge_labels=nx.get_edge_attributes(G,'weight'))
	plt.show()
	return
def draw_partitions(G, partitions):
	'''
	draw the networkx graph  and specify different communities with different colors

	Parameters
	----------

	G : networkx
		original graph
	
	partitions : List
		consists of nodes in each community

	Returns:
	--------

	No return
	'''
	color_map = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']
	color_index = 0
	partition_color = {}
	for partition in partitions:
	    for node in partition:
	        partition_color[node] = color_map[color_index]
	    color_index += 1
	    color_index = color_index % len(color_map)
	node_color = [partition_color.get(node, 'o') for node in G.nodes()]
	nx.draw_spring(G, node_color=node_color, with_labels=True)
	plt.show()

def obtain_degree_inter_cmty(G,nodes_list):
	'''
	Returns the degrees of nodes  in  'nodes_list' and the nodes pairs that have edges between them

	Parameters
	----------

	G : networkx graph
		original networkx which the nodes in 'nodes_list' belong to
	
	nodes_list : List
		A list contians some nodes of graph 'G'
	
	Returns
	-------

	degrees_of_nodes : List
		a list consists of degrees of each nodes in 'nodes_list'
		[[node1,degree],[node2,degree],...]
	
	edges_list : List	
		a list consists of nodes pairs which have an edge between them
		[[node1,node3],[node2,node7],....]
	
	'''
	print "obtain inter degree of the cmty"
	degrees_of_nodes = [] 
	edges_list = []
	for node in nodes_list:
		temp = nx.neighbors(G,node)
		#internal degree of the node in community
		neighbors_in_cmpty = [i for i in temp if i in nodes_list]
		degrees_of_nodes.append([node,len(neighbors_in_cmpty)])
		for j in neighbors_in_cmpty:
			edges_list.append([node,j])
		#degrees_of_nodes.append(len(temp))
	
	print "obtain inter degree of the cmty...ok"
	return degrees_of_nodes,edges_list

def obtain_degree_extern_cmty(G,nodes_list):
	'''
	Return the number of edges between nodes in 'nodes_list' and rest nodes of G

	Parameters
	----------

	G : networkx graph
		original graph that nodes in 'nodes_list' belongs to
	
	nodes_list : List
		some nodes in nodes of graph G

	Returns
	-------

	degrees_of_nodes : Int
		the number of edges between the nodes in 'nodes_list' and the rest nodes of 'G'  

	'''
	print "obtain extern degree of cmty"
	degrees_of_nodes = 0
	for node in nodes_list:
		temp = nx.neighbors(G,node)
		for i in temp:
			if i not in nodes_list:
				degrees_of_nodes += 1
	print "obtain extern degree of cmty....ok"
	return degrees_of_nodes


def obtain_clustering_coefficient(G,nodes_list):
	'''
	Returns the clustering coefficient of node in nodes list

	Parameters
	----------

	G : networkx graph
		original graph that nodes in 'nodes_list' belongs to
	
	nodes_list : List
		some nodes in nodes of graph G

	Returns
	-------
	
	cc : List
		centrality list
	'''
	dic_cc = nx.clustering(G,nodes_list)
	cc = []
	for k in dic_cc:
		cc.append(dic_cc[k])
	cc = sorted(cc)
	return cc

def obtain_between_centrality(G,edges_list):
	'''
	Returns betweenness centrality 

	Parameters
	----------

	G : networkx graph
		original graph that nodes in 'edegs_list' belongs to
	
	edges_list : List
		some nodes pairs which have edges between them
	
	Returns
	-------
	
	bc_list : List
		betweenness centrality list
	
	'''
	#1. creat graph with nodes list
	g = nx.Graph()
	for item in edges_list:
		g.add_edge(item[0],item[1])
	bc = nx.betweenness_centrality(g)
	bc_list = []
	for key in bc:
		bc_list.append(bc[key])
	return bc_list

def obtain_midian_list(value_list):
	'''
	Returns the midian item of the 'value_list'

	Parameters
	----------
	
	value_list : List
		some numbers 
	
	Returns
	-------
	
	midian : float
		midian number of the list
	'''
	value_list = sorted(value_list)	
	half = len(value_list) // 2
	midian = float(value_list[half] + value_list[~half]) / 2

	return midian
	
def obtain_triangles_count(G,nodes_list):
	return nx.triangles(G,nodes_list)

def obtain_feature_of_cmty(G,SG,nodes_list,throd):
	'''
	obtain the feature of the community
	Return: the community feature
	Return type: list which consists of:
		1. outdegree of the community
		2. number of nodes
		3. number of edges
		4. maxmiun degree 1th,2th and 3th
		5. average degree of community
		6. midian degree
		7. density of community
		8. triangles number of the maximun degree
		9. modularity

		10. maxmiun bs contained 1th,2th and 3th
		11. average bs
		12. midian bs
		13. average cc
		14. midian cc.
	'''
	feature = []	
	#obtain the outdegree of the community
	outdegree = obtain_degree_extern_cmty(G,nodes_list)
	feature.append(outdegree)

	#1.calculate number of nodes in community
	feature.append(len(nodes_list))

	#degree of the nodes
	degree_nodes,edges = obtain_degree_inter_cmty(G,nodes_list)

	#2. count the number of edges in community
	degree_list = [item[1] for item in degree_nodes]
	edges_count = sum(degree_list) / 2
	feature.append(edges_count)

	#3.obtain the max five maxmiun value of degree
	degree_nodes = sorted(degree_nodes,key=lambda x:x[1],reverse = True)
	for i in range(5):
		feature.append(degree_nodes[i][1])

	#average and midian degree
	average_degree = float(sum(degree_list))/len(degree_list)	
	midian_degree = obtain_midian_list(degree_list)
	#feature.append(average_degree)
	feature.append(midian_degree)

	#density of the community
	d = float(2 * edges_count) / (len(nodes_list) *(len(nodes_list) - 1))
	feature.append(d)
	
	#max_degree_nodes_list = []

	#for i in range(int(throd * 0.75)):
	#	max_degree_nodes_list.append(degree_nodes[i][0])
	#triangles_count = obtain_triangles_count(G,max_degree_nodes_list)
	#for k in triangles_count:
	#	feature.append(triangles_count[k])

	#4.calculate betweenness centrality 
	between_centrality_list = obtain_between_centrality(G,edges)
	midian_bs = obtain_midian_list(between_centrality_list)

	#max bs 
	for i in range(int(throd * 0.75)):
		feature.append(between_centrality_list[i])
	average_bs = float(sum(between_centrality_list)) / len(between_centrality_list)
	#feature.append(average_bs)
	feature.append(midian_bs)

	##modularity
	#Nodes = snap.TIntV()
	#for nodeId in nodes_list:
	#	    Nodes.Add(nodeId)
	#modularity = snap.GetModularity(SG,Nodes) 
	#feature.append(modularity*1000)

	#5 calculate clustering coefficients
	cc = obtain_clustering_coefficient(G,nodes_list)
	average_cc = float(sum(cc)) / len(cc)
	midian_cc = obtain_midian_list(cc)
	#feature.append(average_cc)
	feature.append(midian_cc)
		
	return feature

def normalize_cmty_feature(feature):
	'''
	Return the normalized feature of community
	value = (value - MInVale) / (MaxValue - MinValue)

	Parameters
	----------

	feature : List , elment type: float
		the feature of the community
	
	Returns
	-------

	normalized_feature : List , elment type: float
		Return the feature whose elements is normalized
	'''

	feature = sorted(feature)
	maxvalue = feature[-1]
	minvalue = feature[0]
	temp = maxvalue - minvalue
	for i in range(len(feature)):
		feature[i] = (feature[i] - minvalue) * 1.0 / temp 
	return feature
		

def obtain_edges_between_cmty(edges_list,s_nodes,d_nodes):
	edges_number = 0
	for i in s_nodes:
		for j in d_nodes:
			item = (i,j)
			item_1 = (j,i)
			if item in edges_list or item_1 in edges_list:
				edges_number += 1
	return edges_number
	

def merge_small_community(G,rest_small_cmty,new_cmty_list):
	'''
	Return the new communities after merging the small communities to the community which is nearest to them 

	Parameters
	----------

	G : networkx graph
		original graph
	
	rest_small_cmty : List
		consist of some communities in which the nodes are less than threshold
		[[node1,node2,...],[node3,node4,...],...,[node_x,node_y,node_z,...]]
	
	new_cmty_list : List
		consists of some communities in which the nodes are more than threshold
		[[node1,node2,...],[node3,node4,...],...,[node_x,node_y,node_z,...]]

	Returns
	-------

	new_cmty_list : List
		consists of some communities in which the nodes are more than threshold
		[[node1,node2,...],[node3,node4,...],...,[node_x,node_y,node_z,...]]
	'''

	print "merge small community....."
	edges_list = G.edges()
	discard_count = 0
	merge_order = []
	for i in range(len(rest_small_cmty)):
		distance = []
		for j in range(len(new_cmty_list)):
			temp =  obtain_edges_between_cmty(edges_list,rest_small_cmty[i],new_cmty_list[j])
			#print "%i <--> %i : %d" % (i,j,temp)
			distance.append([j,temp])
		distance = sorted(distance,key=lambda x:x[1],reverse=True)
		print distance
		if distance[0][1] == 0:
			print rest_small_cmty[i]
			discard_count += 1
			continue
		#join the i into j
		merge_order.append([i,distance[0][0]])
	
	print "merge list: ",
	print merge_order
	for item in merge_order:
		for node in rest_small_cmty[item[0]]:
			new_cmty_list[item[1]].append(node)
	print "after merging,the number of the new communities: %d" % len(new_cmty_list)	
	print "discard %d small communitis" % discard_count
	print "merge small community....ok!!"
	return new_cmty_list

def obtain_cmty_feature_array(G,SG,cmty_list,throd_value):
	'''
	Return the features of communities in the 'cmty_list' in graph 'G'

	Parameters
	----------

	G : networkx graph
		original networkx graph

	SG : Snap graph
		the Snap graph generated with original networkx 'G'

	cmty_list : List
		consists of the communities detected by using detection method in G1 and every community contains some nodes
	[[node1,node3,...],[node2,node5,...],...]	

	throd_value : Int
		the threshold of the number of nodes in the eligible community
	
	Returns:
	--------

	eligible_cmty_list : List
		consists of communities in which nunber of nodes is  more than the threshold
		[[node1,node3,...],[node2,node5,...],...]	
	
	feature : List
		consists of the features of communities in 'eligible_cmty_list'
		[[feature_of_community_1],[feature_of_community_2],...]	
	'''	
	print "obtain cmty feature array"
	feature = [] 
	eligible_cmty_list = []
	rest_small_cmty = []
	new_cmty_list = []
	for cmty in cmty_list:
		if len(cmty) <= throd_value:
				rest_small_cmty.append(cmty)
		else:
			new_cmty_list.append(cmty)
	
	#neglect the small size community
	eligible_cmty_list = new_cmty_list

	
	##join the small community into community with which it is most connected
	#if len(rest_small_cmty) > 0:
	#	eligible_cmty_list = merge_small_community(G,rest_small_cmty,new_cmty_list)
	#else:
	#	eligible_cmty_list = new_cmty_list
	#print "hanld the cmty with throd..ok"
	loop_index = 0
	for cmty in eligible_cmty_list:
		print "cmty: %d" % loop_index
		print "length: %d" % len(cmty) 
		loop_index += 1
		temp = obtain_feature_of_cmty(G,SG,cmty,throd_value)
		feature.append(temp)
		sys.stdout.flush()
	print "obtain cmty feature array finished"
	return eligible_cmty_list,feature

def euclidean_metric(sg1_feature_list,sg2_feature_list):
	'''
	Return euclidean distance of two vectors in matrixes
	
	Parameters
	----------

	sg1_feature_list , sg2_feature_list : List
		consists of features of communities
		[[feature_of_community_1],[feature_of_community_2],...]	
	
	Returns
	-------
	distance[0][0] : float
		euclidean distance of two vectors in matrixes
	'''
	X = [] 
	Y = []
	X.append(sg1_feature_list)
	Y.append(sg2_feature_list)
	distance = euclidean_distances(X,Y) 
	return distance[0][0]
	
def euclidean_distance(sg1_feature_list,sg2_feature_list):
	'''
	Return euclidean distance of two vectors from the 'sg1_feature_list' and 'sg2_feature_list'

	Parameters
	----------

	sg1_feature_list , sg2_feature_list : List
		consists of features of communities
		[[feature_of_community_1],[feature_of_community_2],...]	
	
	Returns
	-------
	distance[0][0] : float
		euclidean distance of two vectors in matrixes

	'''
	count = len(sg1_feature_list)
	x = sg1_feature_list
	y = sg2_feature_list
	temp = 0.0
	for i in range(count):
		temp += (float(x[i] - y[i]))**2
	distance = temp ** 0.5	
	return distance
	
		

def obtain_score_between_features(sg1_feature_list,sg2_feature_list,method = euclidean_metric):
	'''
	calculate the similarity score between features by algritom specified method

	Parameters
	----------

	sg1_feature_list , sg2_feature_list : List
		consists of features of communities
		[[feature_of_community_1],[feature_of_community_2],...]	
	
	method : pointer
		specified the method to calculate the distance between features of communities

	Returns
	-------
	
	scroe_list : List
		contain the distances between each pairs of features of communities 
		[
			[community_1_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
			[community_2_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
			....
		]
	
	'''
	big_feature_list = sg1_feature_list if len(sg1_feature_list) >= len(sg2_feature_list) else sg2_feature_list
	score_list = [[] for i in range(len(big_feature_list))]
	small_feature_list = sg1_feature_list if len(sg1_feature_list) < len(sg2_feature_list) else sg2_feature_list
	for index in range(0,len(big_feature_list)):
		s_feature = big_feature_list[index]
		for d_feature in small_feature_list:
			score = method(s_feature,d_feature)
			score_list[index].append(score)
	return score_list

def calculate_common_nodes_between_cmties(s_nodes_list,d_nodes_list):
	'''
	calculate the number of the nodes existed in both s_nodes_list and d_nodes_list

	Parameters
	----------

	s_nodes_list , d_nodes_list : List
		consists of nodes index
	
	Returns
	-------

	common_nodes_rate : float , less than 1
		the partition of the common nodes in small nodes list

	common_nodes_list : List
		consists of nodes which appear in both 's_nodes_list' and 'd_nodes_list'
		
	'''
	if len(s_nodes_list) == 0 or len(d_nodes_list) == 0:
		return 0
	small_node_list = s_nodes_list if len(s_nodes_list) <= len(d_nodes_list) else d_nodes_list
	big_node_list = s_nodes_list if len(s_nodes_list) > len(d_nodes_list) else d_nodes_list
	common_nodes_list = []
	total_count = len(small_node_list) 
	for node in small_node_list:
		if node in big_node_list:
			common_nodes_list.append(node)
	
	print "source cmty: ",
	print s_nodes_list[:8]
	print "dest cmty: ",
	print d_nodes_list[:8]
	common_count = len(common_nodes_list)
	print "common nodes count: %d" % common_count
	print "small length: %d" % len(small_node_list)
	common_nodes_rate = float(common_count)/total_count 
	return common_nodes_rate, common_nodes_list

def repeated_eavalute_accuracy_by_feature(G1,G2,limit_cmty_nodes = 10,method = euclidean_metric,detect_method = cnm.community_best_partition_with_limit):

	'''
	Return the matched pairs of communities from the 'G1' and 'G2' and calculate the matching accuracy rate between the communities detected by the method specified by 'detect_methond' 	

	Parameters
	----------
	G1,G2 : networkx graph
		original networkx graph

	limit_cmty_nodes : int
		the minimum number of nodes in a community
	
	method : pointer
		point the  function of calculating distance bewteen communities' features

	detect_method : pointer
		point the  function of detecting tht communities in graphes 

	Returns
	-------

		rate : float	
			the accuracy rate of matched pairs of communities

		left_Graph,right_Graph : networkx graph
			specified which graph the left communities or right communities in 'matched_index'  belong to 
			example: matched_index = [[1,3],[5,6],...]. the community of index 1 is in left_Graph and the community of index 3 is in right_Graph 

		left_cmty_list,right_cmty_list : List
			consist of the communities in left Graph and right Graph
			[[community_1],[community_2],....]
		
		matched_index : List
			consists of matched index of communities between the left Graph and right Graph
			[[1,3],[4,5],...]

		scroe_list : List
			contain the distances between each pairs of features of communities 
			[
				[community_1_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
				[community_2_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
				....
			]
		
	'''
	#print "sample rate: %.4f" % sample_rate
	#print "limit cmty nodes: %d" % limit_cmty_nodes


	# The type of SG1 and SG2 is the type Snap needs 
	# SG1_ret_list and SG2_ret_list is the result of the detection of graph
	SG1,SG2,SG1_ret_list,SG2_ret_list = community_detect_graph(G1,G2,detect_method = detect_method)

	# The SG1_rest_small_cmty, same as SG2_rest_small_cmty, holds the communities in which the nodes is lower than the throd. The SG1_new_cmty_list just includes the eligible communities.
	SG1_eligible_cmty_list,SG1_features_list = obtain_cmty_feature_array(G1,SG1,SG1_ret_list,limit_cmty_nodes)
	SG2_eligible_cmty_list,SG2_features_list = obtain_cmty_feature_array(G2,SG2,SG2_ret_list,limit_cmty_nodes)
	score_list = obtain_score_between_features(SG1_features_list,SG2_features_list,method = method)

	rate,left_Graph,right_Graph,left_cmty_list,right_cmty_list,matched_index = calculate_accuracy_rate_by_feature(G1,SG1_eligible_cmty_list,G2,SG2_eligible_cmty_list,score_list,SG1_features_list,SG2_features_list)

	return rate,left_Graph,right_Graph,left_cmty_list,right_cmty_list,matched_index


def calculate_accuracy_rate_by_feature(SG1,SG1_new_cmty,SG2,SG2_new_cmty,score_list,SG1_feature,SG2_feature,throd_value = 0.75):
	'''	
	calculate the accuracy rate of matching communities between graphes

	Parameters
	----------

	SG1 ,SG2 : networkx graph
		original netwrokx graph
	SG1_new_cmty , SG2_new_cmty : List
		the communities list of the SG1 graph and SG2 graph
	
	scroe_list : List
		contain the distances between each pairs of features of communities 
		[
			[community_1_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
			[community_2_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
			....
		]
	
	SG1_feature,SG2_feature : List
		consists of features of communities
		[[feature_of_community_1],[feature_of_community_2],...]	
	
	throd_value : float
		the threshold of accuracy rate of the nodes between the matched communities 
		
	Returns
	-------

	accuracy_rate : float
		the accuracy rate of the matched communities

	big_G : networkx graph
		the number of communities in this graph is greater

	samll_G : networkx graph
		the number of communities in this graph is smaller 

	big_new_cmty : List
		the communities list in 'big_G'

	small_new_cmty : List
		the communities list in 'small_G'
	
	matched_index : List
		contains the pairs index of communities. 
		The left index of pairs is the community from the big_G graph 
		The right index of pairs is the community from the small_G graph 

	return the result of the matched communities index
	'''
	matched_count = 0
	unmatched_count = 0
	matched_index = []

	big_new_cmty,big_G = (SG1_new_cmty,SG1) if len(SG1_new_cmty) >= len(SG2_new_cmty) else (SG2_new_cmty,SG2)
	small_new_cmty,small_G = (SG1_new_cmty,SG1) if len(SG1_new_cmty) < len(SG2_new_cmty) else (SG2_new_cmty,SG2)
	big_feature = SG1_feature if len(SG1_feature) >= len(SG2_feature) else SG2_feature
	small_feature = SG1_feature if len(SG1_feature) < len(SG2_feature) else SG2_feature

	loop_count = len(big_new_cmty)	
	small_cmty_count = len(small_new_cmty)
	for i in range(0,loop_count):
		print "**************************************************************"
		#obtain the similarity list of ith community with all of the other community
		similarity_list =[(index,score_list[i][index]) for index in range(0,small_cmty_count)]
		#sort the similiarity of ith community such that obtain the most similar one
		similarity_list = sorted(similarity_list,key=lambda x:x[1])

		print "cmty: %d" % i
		print "similarity list: ",
		print similarity_list
		first_flag =True 

		while len(similarity_list) > 0:
			#obtain the similiarity list of the best one similiaried with the ith cmty from big community list
			best_score = similarity_list[0][1]
			C_index = similarity_list[0][0]
			print "best score: %.5f" % best_score
			print "C index: %d"% C_index

			#stored the index of the  first matched community
			if first_flag:
				first_flag = False
				first_matched_index = C_index

			similarity_list.pop(0)
			#obtain the community which is most similar to the community specified by C_index
			dest_similarity_list = []	
			for item in score_list:
				dest_similarity_list.append(item[C_index])
			dest_similarity_list = sorted(dest_similarity_list)
			#print dest_similarity_list
			if best_score > dest_similarity_list[0]:
				continue;
			break
		#no matched if length of the similiarity list is zero,guasee the firsted matched community is the best one
		if len(similarity_list) == 0:
			print "guasee matched: %d  ==>  %d" %(i,first_matched_index)
			print "socre: %.5f" % score_list[i][first_matched_index]
			print "cmty: %d" % i
			print big_new_cmty[i][:10]
			print big_feature[i]
			print "cmty: %d" % first_matched_index 
			print small_new_cmty[first_matched_index][:10]
			print small_feature[first_matched_index]
			#calculate the common node between ith community of SG1 and first matched community of SG2
			temp_rate,common_nodes_list = calculate_common_nodes_between_cmties(big_new_cmty[i],small_new_cmty[first_matched_index]) 
			matched_index.append([i,first_matched_index,len(common_nodes_list)])
			if temp_rate >= throd_value:
				matched_count += 1
				print "matched count: %d" % matched_count
				print "mapping successful!"
			else:
				print "mapping failed"
				unmatched_count += 1
				print "unmatched count: %d" % unmatched_count
			continue
		print "best candidate: %d" % C_index
		temp_rate,common_nodes_list = calculate_common_nodes_between_cmties(big_new_cmty[i],small_new_cmty[C_index]) 
		matched_index.append([i,C_index,len(common_nodes_list)])
		print "rate: %.4f" % temp_rate
		if temp_rate >= throd_value:
			print "mapping successful!"
			matched_count += 1
		else:
			print "mapping failed"
			unmatched_count += 1
		print "matched count: %d" % matched_count
		print "unmatched count: %d" % unmatched_count
	
	accuracy_rate = float(matched_count)/loop_count
	print "total count: %d" % loop_count
	print "accuracy rate: %.5f" % accuracy_rate
	return accuracy_rate,big_G,small_G,big_new_cmty,small_new_cmty,matched_index

def deepwalk_map_prob_maxtrix(small_G,long_G,dimensions):
	'''
	Return the matched nodes pairs between the small_G graph and long_G graph	

	Parameters
	----------

	small_G : networkx graph
		the number of nodes in this graph is smaller

	long_G : networkx graph
		the number of nodes in this graph is greater 
	
	dimensions : int
		specified the dimensions of each node' feature in graph 

	Returns
	-------

	matches : List 
		consist of matched nodes pairs. 
		The node specified by left index in pair is from the small_G Graph  
		The node specified by right index in pair is from the small_G Graph  

		[[1,2],[32,3],...]: node 1 is from small_G graph, node 2 is from long_G graph
	
	'''
	print "detect seed list with deepwalk....."
	small_node,long_node,P = dm.map_prob_maxtrix(small_G,long_G,dimensions=dimensions)
	count = 0
	long_loop_count = long_G.order()
	
	matches = []

	for i in range(len(small_node)):
		#obtain ith row
		similarity_list =[(index,P[i][index]) for index in range(long_loop_count)]
		similarity_list = sorted(similarity_list,key=lambda x:x[1],reverse = True)
		while len(similarity_list) > 0:
		#record the matched nodes obtained by deepwalk	
			#obtain the similiarity list of the best one similiaried with the ith cmty from big community list
			best_score = similarity_list[0][1]
			C_index = similarity_list[0][0]

			similarity_list.pop(0)

			dest_similarity_list = []	
			for item in P:
				dest_similarity_list.append(item[C_index])

			dest_similarity_list = sorted(dest_similarity_list,reverse=True)
			if best_score < dest_similarity_list[0]:
				continue;
			break

		if len(similarity_list) == 0:
			#not finding the best matched node
			continue
		
		#record the nodes pairs which is same one node judeged by algrithm
		if best_score >= 1.0:
			matches.append((small_node[i],long_node[C_index]))

	return matches

	
def obtain_accuracy_rate_in_matched_cmty(left_graph,left_cmty_list,right_graph,right_cmty_list,matched_cmty_index_pairs,dimensions=65,seed_matching_method = dm.bipartite_matching):
	'''
		There are 5 stages to match the nodes in different graph:
		1. Detection the community for the graphes
		2. Matching the communities by using their features 
		3. Pre-process stage : apply the nodes matching method, deepwalk&CPD or bipartitie_matching to the subgraphes generated wiht the matched communities pairs and obtain the  matched nodes pairs for the communities pairs
		4. Edges-credibility stage : With the matched nodes pairs obtained in stage 3 as the input to this stage, using the edges credibility algritom to detect the real seed nodes pairs as the output of this stage   
		5. Refine stage : With the real seed nodes pairs obtained in stage 4, applying the propagation algritom to match the rest nodes of the different subgraphes.
	
		Under the 5 stages, this function will return the matched nodes pairs obtained in each stages except the first stage.

		Parameters
		----------

		left_graph,right_graph : networkx graph
			specified which graph the left communities or right communities in 'matched_cmty_index_pairs'  belong to 
			example: matched_cmty_index_pairs = [[1,3],[5,6],...]. the community of index 1 is in left_graph and the community of index 3 is in right_graph 

		left_cmty_list,right_cmty_list : List
			consist of the communities in left Graph and right Graph

		dimensions : int
			specified the dimensions of each nodes' features in graph using by deepwalk&CPD algritom 

		seed_matching_method : pointer
			specified the matching nodes pairs method
			
		Returns
		-------

		matched_nodes_number: list
			Four elments in each item of this list is list and each elment consists of 8 numbers :
			 1. the number of all nodes in the  matched communities and the number of same nodes bewteen matched communities
			 2. the number of all nodes obtained by deepwalk and the number of the same nodes
			 3. the number of the all seed nodes obtained by edge_consistency_method and the number of the real seed nodes
			 4. the number of all nodes obtained by refine stage and the number of the real same nodes pairs

			 [[100,90,80,75,75,50,100,90],[100,90,80,75,75,50,100,90],.....]

		

		pre_process_seed_rate : List,elment type :float
			the ratio of the number of same nodes pairs to the number of the all  nodes pairs for the each matched communities pair in pre_process stage. 
		
		pre_process_seed_nodes_list : List ,elment type : List

			consists of nodes pairs obtained in pre_process stage for each communities pair.
			form as following:
			[
				[[1,4],[2,3],...],
				[[90,48],[70,40],....],
				...
			]
				
		refine_nodes_rate : List ,elment type :float
			the ratio of the number of same nodes pairs to the number of the all  nodes pairs for the each matched communities pair in refine stage. 

		refine_nodes_matches_list : List, elment type: List
			consists of nodes pairs obtained in refine stage for each communities pair.
			form as following:
			[
				[[1,4],[2,3],...],
				[[90,48],[70,40],....],
				...
			]
		
		z_score_list : List, elment type : float
			consists of the z_score for each communities pairs

	'''
	matched_nodes_number = []
	pre_process_seed_rate = []
	pre_process_seed_nodes_list = []
	refine_nodes_rate = []
	refine_nodes_matches_list = []
	z_score_list = []
	for item in matched_cmty_index_pairs:
		left_index = item[0]
		right_index = item[1]
		common_nodes = item[2]
		original_size = len(left_cmty_list[left_index]) if len(left_cmty_list[left_index]) <= len(right_cmty_list[right_index]) else len(right_cmty_list[right_index])

		print "%i -> %i common nodes: %d" % (left_index,right_index,common_nodes)
		if common_nodes == 0:
			continue
		#tansfrom the community to the graph
		G1 = left_graph.subgraph(left_cmty_list[left_index])
		G2 = right_graph.subgraph(right_cmty_list[right_index])

		#judge the small number of nodes of graph
		small_G = G1 if G1.order() <= G2.order() else G2
		long_G = G1 if G1.order() > G2.order() else G2

		
		#mapping the nodes between the communities
		#pre-process stage
		print "pre-process stage is begining...."


		if seed_matching_method == dm.bipartite_matching:
			matches = seed_matching_method(small_G,long_G)
		if seed_matching_method == deepwalk_map_prob_maxtrix:
			matches = seed_matching_method(small_G,long_G,dimensions=dimensions)

		deepwalk_common_nodes_list = []	
		deepwalk_matched_nodes_size = len(matches)
		deepwalk_real_seed_count = 0
		for item in matches:
			if item[0] == item[1]:
				deepwalk_real_seed_count += 1
				deepwalk_common_nodes_list.append(item[0])
		if deepwalk_real_seed_count == 0:
			print "no one is right in the matched obtained by deepwalk"
			pre_process_list = []
			pre_process_count = 0
			pre_process_rate = 0.0
			refine_match_nodes = []
			refine_real_matched_nodes_count = 0
			refine_rate = 0.0
			z_score_list.append(0)

			pre_process_seed_rate.append(pre_process_rate)
			pre_process_seed_nodes_list.append(pre_process_list)
			print "pre-process stage accuracy rate: %.2f" % pre_process_rate
			print "pre-process stage has completed!"

			refine_nodes_rate.append(refine_rate)
			refine_nodes_matches_list.append(refine_match_nodes) 
			print "refine stage accuracy rate: %.2f" % refine_rate
			print "refine stage has completed!"

			matched_nodes_number.append([original_size,common_nodes,deepwalk_matched_nodes_size,deepwalk_real_seed_count,len(pre_process_list),pre_process_count,len(refine_match_nodes),refine_real_matched_nodes_count])
			continue

		#pre-process stage: seed nodes detection
		pre_process_list,pre_process_count,pre_process_rate,z_score = obtain_seed_with_edges_credibility(matches,small_G,long_G,deepwalk_common_nodes_list)

		pre_process_seed_rate.append(pre_process_rate)
		pre_process_seed_nodes_list.append(pre_process_list)
		z_score_list.append(z_score)

		print "pre-process stage accuracy rate: %.2f" % pre_process_rate
		print "%d : %d" % (len(pre_process_list),pre_process_count) 
		if len(pre_process_list) != 0:
			print "temp rate: %.2f" % (pre_process_count*1.0 / len(pre_process_list))
		print "pre-process and edge credibility stage has completed!"

		#refine stage: refine the remain nodes in the graph with seed list
		print "refine stage start ...."
		refine_match_nodes,refine_real_matched_nodes_count,refine_rate = match_propagation(pre_process_list,small_G,long_G)
		
		refine_nodes_rate.append(refine_rate)
		refine_nodes_matches_list.append(refine_match_nodes) 
		print "refine stage accuracy rate: %.2f" % refine_rate
		print "%d : %d" % (len(refine_match_nodes),refine_real_matched_nodes_count) 
		print "refine stage has completed!"
		
		matched_nodes_number.append([original_size,common_nodes,deepwalk_matched_nodes_size,deepwalk_real_seed_count,len(pre_process_list),pre_process_count,len(refine_match_nodes),refine_real_matched_nodes_count])
		
	return matched_nodes_number,pre_process_seed_rate,pre_process_seed_nodes_list,refine_nodes_rate,refine_nodes_matches_list,z_score_list
			
def draw_subgraph_after_deepwalk(small_G,long_G,matched_nodes_pairs,deepwalk_common_nodes_list):
	'''
	Draw the subgraph after pre_process stage----deepwalk&CPD.

	'''
	partitions = []	
	partitions.append([node for node in matched_nodes_pairs if node not in deepwalk_common_nodes_list])
	partitions.append([matched_nodes_pairs[node] for node in matched_nodes_pairs if matched_nodes_pairs[node] not in deepwalk_common_nodes_list])
	sub_nodes_list = partitions[0] + partitions[1]
	subgraph_small = small_G.subgraph(sub_nodes_list)
	draw_partitions(subgraph_small,partitions)
	partitions_matched = []
	partitions_matched.append([node for node in matched_nodes_pairs if node not in deepwalk_common_nodes_list])
	partitions_matched.append([matched_nodes_pairs[node] for node in matched_nodes_pairs if matched_nodes_pairs[node] not in deepwalk_common_nodes_list])
	sub_nodes_list_matched = partitions_matched[0] + partitions_matched[1]
	subgraph_big = long_G.subgraph(sub_nodes_list_matched)
	draw_partitions(subgraph_big,partitions_matched)
	return

def plot_z_score(z_score_list):
	'''
	Plot the consistency sequence

	Parameters
	----------

	z_score_list : List, elment type : float
		consists of the z_score for each communities pairs

	Returns 
	-------

		No returns
	'''
	fig, ax = plt.subplots()
	ax.plot(z_score_list, '.-')
	ax.set_xlabel('Community Index',fontsize = 17)
	ax.set_title('Each Matching Z_Score',fontsize=18)
	ax.set_ylabel('Z_score',fontsize = 17)
	plt.tight_layout()
	plt.show()



def main():
	if len(sys.argv) < 7:
		print "usage: ./deepmatching_for_cmty.py [filename] [sample rate]  [community size threshold] [loop count] [distance function = 1] [remarks]"
		return -1
	sample_rate = float(sys.argv[2])
	throd_value = int(sys.argv[3])
	repeated_count = int(sys.argv[4])
	if(sys.argv[5] == "1"):
		method_select = euclidean_distance
	else:
		method_select = euclidean_metric

	filename = "cmty_matching_with_sample_%.2f_repeat_%d_cmty_throd_%d.txt"%(float(sys.argv[2]),repeated_count,throd_value)
	print "result will be recorded in %s"%filename
	df = open(filename,"w")
	df.write("########################################################################\n")
	df.write("date: %s\n" % ti.strftime("%Y-%m-%d %H:%M:%S",ti.localtime(ti.time())))
	df.write("remarks: %s\n" % sys.argv[6])
	df.write("dataset: %s\n" % sys.argv[1])
	df.write("sample: %.2f\n" % float(sys.argv[2]))
	df.write("similarity function: %s" % "euclidean_distance")
	df.write("throd of the community: %d\n" % throd_value)
	df.write("repeated loop count: %d\n" % repeated_count)
	df.flush()

	#load graph form file
	nx_G = load_graph_from_file(sys.argv[1],delimiter = ' ')

	df.write("Graph Infomation: nodes %d edges %d\n" % (len(nx_G.nodes()),len(nx_G.edges())))
	df.flush()

	print "begin to execute the 5 stage....."	
	sum_acc = 0.0
	rate_list = []
	dimensions = 160 
	for i in range(repeated_count):
		print "->%d"%i ,
		sys.stdout.flush()
		G1,G2 = bi_sample_graph(nx_G,sample_rate)

		#obtain the pairs of index of the matched communities
		old_rate,left_graph,right_graph,left_cmty_list,right_cmty_list,matched_index = repeated_eavalute_accuracy_by_feature(G1,G2,limit_cmty_nodes = throd_value)
		#record the small commnuity size
		df.write("%ith loop:\n"%i)
		df.write("G1 cmty length distribution: ")
		for index in left_cmty_list:
			df.write("%i " % (len(index)))
			df.flush()
		df.write("\n")
		df.write("G2 cmty length distribution: ")
		for index in right_cmty_list:
			df.write("%i " % (len(index)))
			df.flush()
		df.write("\n")

		rate_list.append(old_rate)
		sum_acc += old_rate
		
		# the dimensions of feature of the nodes obtained by deepwalk 
		matched_nummber_nodes,pre_process_seed_rate,pre_process_seed_nodes_list,refine_rate,refine_match_nodes_list,z_score_list = obtain_accuracy_rate_in_matched_cmty(left_graph,left_cmty_list,right_graph,right_cmty_list,matched_index,dimensions) 

		df.write("dimensions: %d\n" % dimensions)
		df.write("deepwalk results[cmty-deepwalk-seed]: ")
		for item in matched_nummber_nodes:
			df.write("%d-%d-%d-%d-%d-%d-%d-%d " % (item[0],item[1],item[2],item[3],item[4],item[5],item[6],item[7]))
			df.flush()
		df.write('\n')
		df.flush()
		#finding the seed nodes and calculating the accuracy rate
		df.write("pre-process stage seed accuracy rate: ")
		for item in pre_process_seed_rate:
			df.write(" %.5f" % item)
			df.flush()
		df.write('\n')
		df.flush()



		df.write("refine stage seed accuracy rate: ")
		for item in refine_rate:
			df.write(" %.5f" % item)
			df.flush()
		df.write('\n')
		df.flush()

		plot_z_score(z_score_list)
			
	df.write("accuracy rate array of matched communities pairs: ")
	df.flush()
	for item in rate_list:
		df.write("%.4f " % item)
		df.flush()
	df.write("\n")
	df.write("arverage: %.5f\n"% (sum_acc / repeated_count))
	df.write("########################################################################\n")
	df.flush()

	print "mapping community finished!!"
	return 0
	

	
if __name__ == "__main__":
    main()

