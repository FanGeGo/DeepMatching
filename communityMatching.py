#!/usr/bin/python
import numpy as np
import community as cmty
import os
import time as ti
import math
import networkx as nx
from scipy.io import loadmat
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import time
from graph_matching import * 
import sys
import cnm
from sklearn.metrics.pairwise import *
import snap
import deepMatching as dm
from itertools import izip
from credibility import *
from refinement import *
from bimatching.sparse_munkres import munkres

def transform_snap_to_networkx(SG):
        '''
        Transforming the graph of snap specified by the @SG to the graph of networkx
        Return the graph of networkx as the result of this function

        Parameters
        ----------

        SG : snap graph

        Returns
        -------

        G_networkx : networkx graph
                networkx graph generated by transforming the Snap graph 

        '''
        G_networkx = nx.Graph()
        #add the nodes from the SG
        for node in SG.Nodes():
                G_networkx.add_node(node.GetId())
        #add edges to the G_networkx
        for i in G_networkx.nodes():
                for j in G_networkx.nodes():
                        if SG.IsEdge(i,j):
                                G_networkx.add_edge(i,j)
        return G_networkx       
def transform_networkx_to_snap(G):
        '''
        Transforming the graph of networkx specified by the @G to the graph of snap 
        Return the graph of snap as the result of this function

        Parameters
        ----------

        G : netoworkx
                the source networkx graph
        
        Returns
        -------

        TG : snap graph
                snap graph generated by transforming to the networkx graph

        '''
        TG = snap.TUNGraph.New()
        print "start transform the graph format from the networkx to snap ......"
        start_time = time.time()
        #add the nodes from the G
        print len(G.nodes())
        for node in G.nodes():
                TG.AddNode(node)
        #add edges to the TG
        for edge in G.edges():
                TG.AddEdge(edge[0],edge[1])

        print "transform finished,the Graph nodes: %d\t edges: %d" % (TG.GetNodes(),TG.GetEdges())
        running_time = time.time() - start_time
        print "sample finished,running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
        return TG
        

def bi_sample_graph(nx_G,sample_rate = 0.8):
        '''
                Returns two ntworkx graphes generated by applyin edge sampling process to the fix networkx graph 'nx_G'

        Paratemers
        ----------

        nx_G : networkx graph
                the original graph
        
        sample_rate : float, nonegative and  less than 1
                the edge sampling process apply to original graph to generate two networkx graphes by using the sampling rate  

        Returns
        -------
        G1,G2 : networkx graph
                the subgraph after sampling

        '''
        G1 = sample_graph(nx_G, sample_rate)
        G2 = sample_graph(nx_G, sample_rate)
        return G1,G2
        
def community_detect_graph(G1,G2,detect_method = None,limit_ceil_cmty = 1000):
        '''
        detect community and generate graph with community as nodes and the number edges between communities as weigth between new nodes  

        Parameters
        ----------

        G1,G2 : networkx graph
                original graphes to match 
        
        detect_method : pionter of function
                specified the name of detection method
        
        Returns
        -------

        SG1,SG2 : Snap graph
                Snap graph generated with original networkx graph 'G' 

        SG1_ret_list : List
                consists of the communities detected by using detection method in G1 and every community contains the nodes
        [[node1,node3,...],[node2,node5,...],...]       


        SG2_ret_list : List
                consists of the communities detected by using detection method in G2 and every community contains the nodes
        [[node1,node3,...],[node2,node5,...],...]       

        '''
        print "start sample ......"

        start_time = time.time()
        if detect_method == cnm.community_cnm_with_limit:
                SG1 = transform_networkx_to_snap(G1)
                SG2 = transform_networkx_to_snap(G2)
                print "limit nodes of the community for G1 : %d" % (len(G1.nodes()) / 20)
                SG1_ret_list = detect_method(SG1,len(G1.nodes())/20)
                print "limit nodes of the community for G2 : %d" % (len(G2.nodes()) / 20)
                SG2_ret_list = detect_method(SG2,len(G2.nodes()) /20)
                print "SG1 community size: %d \t SG2 community size :%d " % (len(SG1_ret_list),len(SG2_ret_list))
                running_time = time.time() - start_time
                print "sample finished,running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
                return SG1_ret_list,SG2_ret_list
        if detect_method == cmty.best_partition:
                s1_partition = cmty.best_partition(G1)
                s2_partition = cmty.best_partition(G2)
                SG1_ret_list = []
                SG2_ret_list = []
                for com in set(s1_partition.values()):
                        temp = [nodes for nodes in s1_partition.keys() if s1_partition[nodes] == com]
                        SG1_ret_list.append(temp)
                for com in set(s2_partition.values()):
                        temp = [nodes for nodes in s2_partition.keys() if s2_partition[nodes] == com]
                        SG2_ret_list.append(temp)
                running_time = time.time() - start_time
                print "SG1 community size: %d \t SG2 community size :%d " % (len(SG1_ret_list),len(SG2_ret_list))
                print "sample finished,running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
                SG1 = transform_networkx_to_snap(G1)
                SG2 = transform_networkx_to_snap(G2)
                return SG1_ret_list,SG2_ret_list

        if detect_method == cnm.community_best_partition_with_limit:
                print "ceil size of community : %d " % limit_ceil_cmty
                SG1_ret_list = cnm.community_best_partition_with_limit(G1,limit_ceil_cmty)
                SG2_ret_list = cnm.community_best_partition_with_limit(G2,limit_ceil_cmty)

                print "SG1 community size: %d \t SG2 community size :%d " % (len(SG1_ret_list),len(SG2_ret_list))
                running_time = time.time() - start_time
                print "sample finished,running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
                SG1 = transform_networkx_to_snap(G1)
                SG2 = transform_networkx_to_snap(G2)
                return SG1_ret_list,SG2_ret_list

        if detect_method == cnm.community_cnm:
                SG1 = transform_networkx_to_snap(G1)
                SG2 = transform_networkx_to_snap(G2)
        else:
                SG1 = G1
                SG2 = G2
        SG1_ret_list = detect_method(SG1)
        SG2_ret_list = detect_method(SG2)

        running_time = time.time() - start_time
        print "SG1 community size: %d \t SG2 community size :%d " % (len(SG1_ret_list),len(SG2_ret_list))
        print "sample finished,running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
        
        return SG1_ret_list,SG2_ret_list
         
        
def transform_gml_to_networkx(gml_filename):
        '''
        load the graph from gml file to generate the networkx graph

        Parameters
        ----------

        gml_filename : string
                gml file path
        
        Returns
        -------

        G : networkx
                the networkx graph generated from the gml file  

        '''
        sf = open(gml_filename)
        G = nx.Graph()
        
        line = sf.readline()
        while "edge" not in line:
                line = sf.readline()

        while line != '':
                line = sf.readline()
                if line == '':
                        break
                line = sf.readline()
                if line == '':
                        break
                line = line.split(' ')

                source = int(line[-1][:-1]) + 1
                line = sf.readline()
                if line == '':
                        break
                line = line.split(' ')
                dest = int(line[-1][:-1]) + 1
                G.add_edge(source,dest)
                line = sf.readline()
                if line == '':
                        break
                line = sf.readline()

        sf.close()
        print "load finished,the Graph nodes: %d\t edges: %d" % (len(G.nodes()),len(G.edges()))
        return G


def load_graph_from_file(filename,comments = '#',delimiter = ' '):
        '''
                load the graph from file to generate the networkx graph

        Parameters
        ----------

        filename : string
                contain the nodes and edges of graph
        
        comments : char 
                the line begined with '#' is commentary
        
        delimiter : char
                separate and obstract the nodes or the edges in every vaild line  
        
        Returns
        -------
        
        G : networkx graph
                create the new graph with graph file

        '''
        print "start load graph from file: %s" % filename
        if ".gml" in filename:
                return transform_gml_to_networkx(filename)
        G = nx.Graph()
        start_time = time.time()
        sf = open(filename)
        line = sf.readline()
        while line != '':
                if comments not in line:
                        break
                line = sf.readline()
        while line != '':
                line = line.split(delimiter)
                # plus 1 reprents that the index of nodes is start from 1 at least(For sanp graph ,the nodes' index must start from 1 at least)
                if G.has_edge(int(line[0]) + 1,int(line[1][:-1]) + 1)==False and G.has_edge(int(line[1][:-1]) + 1,int(line[0]) + 1) == False:
                        G.add_edge(int(line[0]) + 1,int(line[1][:-1]) + 1)
                #G.add_edge(int(line[0]),int(line[1][:-1]))
                line = sf.readline()
        running_time = time.time() - start_time
        #print "G nodes: %d \t edges: %d" % (len(G.nodes()),len(G.edges()))
        #print "running time :%d mins %d secs" % (int(running_time / 60),int(running_time % 60))
        print "load graph finish!! "
        sf.close()
        return G

def draw_networkx(G):
        '''
        draw the networkx graph

        Parameters
        ----------

        G : networkx
                original graph
        Returns:
        --------

        No return
        '''
        pos = nx.spring_layout(G)
        nx.draw_networkx_nodes(G,pos);
        nx.draw_networkx(G,pos)
        plt.show()
        return


def draw_networkx_with_weight(G):
        '''
        draw the networkx graph

        Parameters
        ----------

        G : networkx
                original graph
        Returns:
        --------

        No return
        '''

        pos = nx.spring_layout(G)
        nx.draw_networkx_nodes(G,pos);
        nx.draw_networkx_edges(G,pos);
        nx.draw_networkx_labels(G,pos);
        nx.draw_networkx_edge_labels(G,pos,edge_labels=nx.get_edge_attributes(G,'weight'))
        plt.show()
        return
def draw_partitions(G, partitions):
        '''
        draw the networkx graph  and specify different communities with different colors

        Parameters
        ----------

        G : networkx
                original graph
        
        partitions : List
                consists of nodes in each community

        Returns:
        --------

        No return
        '''
        color_map = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']
        color_index = 0
        partition_color = {}
        for partition in partitions:
                for node in partition:
                        partition_color[node] = color_map[color_index]
                color_index += 1
                color_index = color_index % len(color_map)
        node_color = [partition_color.get(node, 'o') for node in G.nodes()]
        nx.draw_spring(G, node_color=node_color, with_labels=True)
        plt.show()

def obtain_degree_inter_cmty(G,nodes_list):
        '''
        Returns the degrees of nodes  in  'nodes_list' and the nodes pairs that have edges between them

        Parameters
        ----------

        G : networkx graph
                original networkx which the nodes in 'nodes_list' belong to
        
        nodes_list : List
                A list contians some nodes of graph 'G'
        
        Returns
        -------

        degrees_of_nodes : List
                a list consists of degrees of each nodes in 'nodes_list'
                [[node1,degree],[node2,degree],...]
        
        edges_list : List       
                a list consists of nodes pairs which have an edge between them
                [[node1,node3],[node2,node7],....]
        
        '''
        degrees_of_nodes = [] 
        edges_list = []
        for node in nodes_list:
                temp = nx.neighbors(G,node)
                #internal degree of the node in community
                neighbors_in_cmpty = [i for i in temp if i in nodes_list]
                degrees_of_nodes.append([node,len(neighbors_in_cmpty)])
                for j in neighbors_in_cmpty:
                        edges_list.append([node,j])
                #degrees_of_nodes.append(len(temp))
        
        return degrees_of_nodes,edges_list

def obtain_degree_extern_cmty(G,nodes_list):
        '''
        Return the number of edges between nodes in 'nodes_list' and rest nodes of G

        Parameters
        ----------

        G : networkx graph
                original graph that nodes in 'nodes_list' belongs to
        
        nodes_list : List
                some nodes in nodes of graph G

        Returns
        -------

        degrees_of_nodes : Int
                the number of edges between the nodes in 'nodes_list' and the rest nodes of 'G'  

        '''
        degrees_of_nodes = 0
        for node in nodes_list:
                temp = nx.neighbors(G,node)
                for i in temp:
                        if i not in nodes_list:
                                degrees_of_nodes += 1
        return degrees_of_nodes

def obtain_clustering_coefficient_distribution(cc,each_step = 0.1):
        new_cc = []
        cc_distribution_list = []
        step = [i*each_step for i in range(int(1/each_step + 1))][1:]

        for base in step:
                count = 0
                for item in cc: 
                        if item < base and item not in new_cc:
                                count += 1 
                                new_cc.append(item)
                cc_distribution_list.append(count)
        return cc_distribution_list
        

        
def obtain_clustering_coefficient(G,nodes_list):
        '''
        Returns the clustering coefficient of node in nodes list

        Parameters
        ----------

        G : networkx graph
                original graph that nodes in 'nodes_list' belongs to
        
        nodes_list : List
                some nodes in nodes of graph G

        Returns
        -------
        
        cc : List
                centrality list
        '''
        dic_cc = nx.clustering(G,nodes_list)
        cc = []
        for k in dic_cc:
                cc.append(dic_cc[k])
        cc = sorted(cc)
        return cc

def obtain_between_centrality(G,edges_list):
        '''
        Returns betweenness centrality 

        Parameters
        ----------

        G : networkx graph
                original graph that nodes in 'edegs_list' belongs to
        
        edges_list : List
                some nodes pairs which have edges between them
        
        Returns
        -------
        
        bc_list : List
                betweenness centrality list
        
        '''
        #1. creat graph with nodes list
        g = nx.Graph()
        for item in edges_list:
                g.add_edge(item[0],item[1])
        bc = nx.betweenness_centrality(g)
        bc_list = []
        for key in bc:
                bc_list.append(bc[key])
        return bc_list

def obtain_midian_list(value_list):
        '''
        Returns the midian item of the 'value_list'

        Parameters
        ----------
        
        value_list : List
                some numbers 
        
        Returns
        -------
        
        midian : float
                midian number of the list
        '''
        value_list = sorted(value_list) 
        half = len(value_list) // 2
        midian = float(value_list[half] + value_list[~half]) / 2

        return midian
        
def obtain_triangles_count(G,nodes_list=None):
        return nx.triangles(G,nodes_list)



def obtain_feature_of_cmty(G,SG,nodes_list,throd):
        '''
        obtain the feature of the community
        Return: the community feature
        Return type: list which consists of:
                1. outdegree of the community
                2. number of nodes
                3. number of edges
                4. maxmiun degree 1th,2th and 3th
                5. average degree of community
                6. midian degree
                7. density of community
                8. triangles number of the maximun degree
                9. modularity

                10. maxmiun bs contained 1th,2th and 3th
                11. average bs
                12. midian bs
                13. average cc
                14. midian cc.
        '''
        features_name_list = ["outdegree","nodes","edges","%d th max degree list"%(int(throd*0.75)),"average degree","midian degree","density","%d th triangles"%(int(throd*0.75)),"modularity","%d th triangles"%(int(throd*0.75)),"average bs","midian bs","%d th cc" % (int(throd * 0.75)),"average cc","midian cc"]
        feature = []    
        #obtain the outdegree of the community
        outdegree = obtain_degree_extern_cmty(G,nodes_list)
        feature.append(outdegree)
        #1.calculate number of nodes in community
        feature.append(len(nodes_list))

        #degree of the nodes
        degree_nodes,edges = obtain_degree_inter_cmty(G,nodes_list)

        #2. count the number of edges in community
        degree_list = [item[1] for item in degree_nodes]
        edges_count = sum(degree_list) / 2
        feature.append(edges_count)

        #3.obtain the max five maxmiun value of degree
        degree_nodes = sorted(degree_nodes,key=lambda x:x[1],reverse = True)
        for i in range(int(throd*0.75)):
                feature.append(degree_nodes[i][1])

        #average and midian degree
        average_degree = float(sum(degree_list))/len(degree_list)       
        midian_degree = obtain_midian_list(degree_list)
        feature.append(average_degree)
        feature.append(midian_degree)

        #density of the community
        d = float(2 * edges_count) / (len(nodes_list) *(len(nodes_list) - 1))
        feature.append(d)
        
        max_degree_nodes_list = []

        for i in range(int(throd * 0.75)):
                max_degree_nodes_list.append(degree_nodes[i][0])
        triangles_count = obtain_triangles_count(G,max_degree_nodes_list)
        for k in triangles_count:
                feature.append(triangles_count[k])

        #4.calculate betweenness centrality 
        between_centrality_list = obtain_between_centrality(G,edges)
        midian_bs = obtain_midian_list(between_centrality_list)


        #max bs 
        for i in range(int(throd * 0.75)):
                feature.append(between_centrality_list[i])
        average_bs = float(sum(between_centrality_list)) / len(between_centrality_list)
        feature.append(average_bs)
        feature.append(midian_bs)

        ##modularity
        Nodes = snap.TIntV()
        for nodeId in nodes_list:
                        Nodes.Add(nodeId)
        modularity = snap.GetModularity(SG,Nodes) 
        feature.append(modularity*1000)

        #5 calculate clustering coefficients
        cc = obtain_clustering_coefficient(G,nodes_list)
        for i in range(int(throd * 0.75)):
                feature.append(cc[i])
        average_cc = float(sum(cc)) / len(cc)
        midian_cc = obtain_midian_list(cc)
        feature.append(average_cc)
        feature.append(midian_cc)
                
        return feature


def obtain_degree_distribution_list(degree_nodes,ceil_value):
        distribution_list = [0 for i in range(ceil_value - 1)] 
        degrees_list = [i[1] for i in degree_nodes]
        have_handle = []
        for i in range(ceil_value - 1):
                if i > degrees_list[0]:
                        break
                distribution_list[i] = degrees_list.count(i) 
        return distribution_list
                


def obtain_basic_feature_of_cmty(G,nodes_list,low_threshold,upper_threshold):
    '''
    1. outdegree of the community
    2. number of nodes
    3. number of edges
    4. maxmiun 10th degree
    5. average degree of community
    6. midian degree
    7. density of community
    '''
    feature = []    
    #obtain the outdegree of the community
    outdegree = obtain_degree_extern_cmty(G,nodes_list)
    feature.append(outdegree)
    feature.append(len(nodes_list))

    #calculate the degree distribution of the nodes
    degree_nodes,edges = obtain_degree_inter_cmty(G,nodes_list)
    #2.1 calculate the degree of the nodes in the community 
    degree_nodes = sorted(degree_nodes,key=lambda x:x[1],reverse = True)
    
    degree_list = [item[1] for item in degree_nodes]
    edges_count = sum(degree_list) / 2
    feature.append(edges_count)

    number_of_top_ten = int(low_threshold)
    top_ten_nodes = [item[0] for item in degree_nodes][:number_of_top_ten]
    average_degree = float(sum(degree_list))/len(degree_list)       
    midian_degree = obtain_midian_list(degree_list)
    average_degree_top_ten = float(sum(degree_list[:number_of_top_ten]))/number_of_top_ten
    midian_degree_top_ten = obtain_midian_list(degree_list[:number_of_top_ten])

    #3 calculate the triangles distribution
    max_nodes_list = []
    for i in range(number_of_top_ten):
        max_nodes_list.append(degree_nodes[i][0])
    #triangles_count = obtain_triangles_count(G,max_nodes_list)
    #triangles = [[key,triangles_count[key]] for key in triangles_count]
    #triangles = sorted(triangles,key=lambda x:x[1],reverse = True)
    #triangles_count = [item[0] for item in triangles]

    #feature.append(triangles_count[0])
    #feature.append(float(sum(triangles_count))/len(triangles_count))
    #median_triangles = obtain_midian_list(triangles_count)
    #triangles_distribution = obtain_degree_distribution_list(triangles,upper_threshold)
    #for k in triangles_distribution:
    #        feature.append(k)



    #add the max degree,mean and midian
    feature.append(average_degree)
    feature.append(midian_degree)
    #feature.append(average_degree_top_ten)
    #feature.append(midian_degree_top_ten)


    for item in range(number_of_top_ten):
        feature.append(degree_list[item])
    
    #density of the community
    d = float(2 * edges_count) / (len(nodes_list) *(len(nodes_list) - 1))
    feature.append(d)

    return feature,top_ten_nodes
      
def obtain_feature_of_cmty_with_degree_distribution(G,nodes_list,throd,ceil_value):
        '''
        obtain the feature of the community
        Return: the community feature
        Return type: list which consists of:
                1. outdegree of the community
                2. number of nodes
                3. number of edges
                4. maxmiun degree 1th,2th and 3th
                5. average degree of community
                6. midian degree
                7. density of community
                8. triangles number of the maximun degree
                9. modularity

                10. maxmiun bs contained 1th,2th and 3th
                11. average bs
                12. midian bs
                13. average cc
                14. midian cc.
        '''
        feature = []    
        #obtain the outdegree of the community
        outdegree = obtain_degree_extern_cmty(G,nodes_list)

        feature.append(outdegree)
        feature.append(len(nodes_list))

        #calculate the degree distribution of the nodes
        degree_nodes,edges = obtain_degree_inter_cmty(G,nodes_list)
        #2.1 calculate the degree distribution of the nodes in the community 
        degree_nodes = sorted(degree_nodes,key=lambda x:x[1],reverse = True)
        degree_list = [item[1] for item in degree_nodes]
        average_degree = float(sum(degree_list))/len(degree_list)       
        #midian_degree = obtain_midian_list(degree_list)
        feature.append(average_degree)
        #feature.append(midian_degree)
        top_ten_nodes = [item[0] for item in degree_nodes][:throd]

        max_nodes_list = []
        for i in range(throd):
                max_nodes_list.append(degree_nodes[i][0])
        
        degree_distribution_list = obtain_degree_distribution_list(degree_nodes[:throd],ceil_value)
        #add the degree distribution to the feature one by one
        for item in degree_distribution_list:
                feature.append(item)

        #2. count the number of edges in community
        degree_list = [item[1] for item in degree_nodes]
        edges_count = sum(degree_list) / 2

        #density of the community
        d = float(2 * edges_count) / (len(nodes_list) *(len(nodes_list) - 1))
        feature.append(d)
        
        #3 calculate the triangles distribution
        triangles_count = obtain_triangles_count(G,max_nodes_list)
        triangles = [[key,triangles_count[key]] for key in triangles_count]
        triangles = sorted(triangles,key=lambda x:x[1],reverse = True)
        triangles_distribution = obtain_degree_distribution_list(triangles,ceil_value)
        for k in triangles_distribution:
                feature.append(k)

        #4.calculate betweenness centrality 
        between_centrality_list = obtain_between_centrality(G,edges)
        between_centrality_list = sorted(between_centrality_list,reverse=True)

        bs_distribution_list = obtain_clustering_coefficient_distribution(between_centrality_list,each_step = 0.001)
        for item in bs_distribution_list:
                feature.append(item)

        #5 calculate clustering coefficients
        cc = obtain_clustering_coefficient(G,max_nodes_list)
        cc_distribution_list = obtain_clustering_coefficient_distribution(cc,0.001)
        for item in cc_distribution_list:
                feature.append(item)
                
        return feature,top_ten_nodes

def normalize_cmty_feature(feature):
        '''
        Return the normalized feature of community
        value = (value - MInVale) / (MaxValue - MinValue)

        Parameters
        ----------

        feature : List , elment type: float
                the feature of the community
        
        Returns
        -------

        normalized_feature : List , elment type: float
                Return the feature whose elements is normalized
        '''

        feature = sorted(feature)
        maxvalue = feature[-1]
        minvalue = feature[0]
        temp = maxvalue - minvalue
        for i in range(len(feature)):
                feature[i] = (feature[i] - minvalue) * 1.0 / temp 
        return feature
                

def obtain_edges_between_cmty(edges_list,s_nodes,d_nodes):
        edges_number = 0
        for i in s_nodes:
                for j in d_nodes:
                        item = (i,j)
                        item_1 = (j,i)
                        if item in edges_list or item_1 in edges_list:
                                edges_number += 1
        return edges_number
        

def merge_small_community(G,rest_small_cmty,new_cmty_list):
        '''
        Return the new communities after merging the small communities to the community which is nearest to them 

        Parameters
        ----------

        G : networkx graph
                original graph
        
        rest_small_cmty : List
                consist of some communities in which the nodes are less than threshold
                [[node1,node2,...],[node3,node4,...],...,[node_x,node_y,node_z,...]]
        
        new_cmty_list : List
                consists of some communities in which the nodes are more than threshold
                [[node1,node2,...],[node3,node4,...],...,[node_x,node_y,node_z,...]]

        Returns
        -------

        new_cmty_list : List
                consists of some communities in which the nodes are more than threshold
                [[node1,node2,...],[node3,node4,...],...,[node_x,node_y,node_z,...]]
        '''

        print "merge small community....."
        edges_list = G.edges()
        discard_count = 0
        merge_order = []
        for i in range(len(rest_small_cmty)):
                distance = []
                for j in range(len(new_cmty_list)):
                        temp =  obtain_edges_between_cmty(edges_list,rest_small_cmty[i],new_cmty_list[j])
                        #print "%i <--> %i : %d" % (i,j,temp)
                        distance.append([j,temp])
                distance = sorted(distance,key=lambda x:x[1],reverse=True)
                print distance
                if distance[0][1] == 0:
                        print rest_small_cmty[i]
                        discard_count += 1
                        continue
                #join the i into j
                merge_order.append([i,distance[0][0]])
        
        print "merge list: ",
        print merge_order
        for item in merge_order:
                for node in rest_small_cmty[item[0]]:
                        new_cmty_list[item[1]].append(node)
        print "after merging,the number of the new communities: %d" % len(new_cmty_list)        
        print "discard %d small communitis" % discard_count
        print "merge small community....ok!!"
        return new_cmty_list


def obtain_cmty_feature_array(G,cmty_list,low_threshold,upper_threshold,feature_method = obtain_basic_feature_of_cmty):
        '''
        Return the features of communities in the 'cmty_list' in graph 'G'

        Parameters
        ----------

        G : networkx graph
                original networkx graph

        cmty_list : List
                consists of the communities detected by using detection method in G1 and every community contains some nodes
        [[node1,node3,...],[node2,node5,...],...]       

        low_threshold : Int
                the low threshold of the number of nodes in the eligible community
        upper_threshold : Int
                the ceil threshold of the number of nodes in the eligible community
        
        Returns:
        --------
        feature : List
                consists of the features of communities in 'eligible_cmty_list'
                [[feature_of_community_1],[feature_of_community_2],...] 
        '''     
        print "obtain cmty feature array"
        feature = [] 
        nonormal_feature = []
        top_ten_nodes_list = []
        for cmty in cmty_list:
                temp,top_ten_nodes = feature_method(G,cmty,low_threshold,upper_threshold)
                nonormal_feature.append(temp)
                temp = normalize_cmty_feature(temp)
                feature.append(temp)
                top_ten_nodes_list.append(top_ten_nodes)
        print "obtain cmty feature array finished"
        return feature,nonormal_feature,top_ten_nodes_list

def euclidean_metric(sg1_feature_list,sg2_feature_list):
        '''
        Return euclidean distance of two vectors in matrixes
        
        Parameters
        ----------

        sg1_feature_list , sg2_feature_list : List
                consists of features of communities
                [[feature_of_community_1],[feature_of_community_2],...] 
        
        Returns
        -------
        distance[0][0] : float
                euclidean distance of two vectors in matrixes
        '''
        X = [] 
        Y = []
        X.append(sg1_feature_list)
        Y.append(sg2_feature_list)
        distance = euclidean_distances(X,Y) 
        return distance[0][0]
        
def euclidean_distance(sg1_feature_list,sg2_feature_list):
        '''
        Return euclidean distance of two vectors from the 'sg1_feature_list' and 'sg2_feature_list'

        Parameters
        ----------

        sg1_feature_list , sg2_feature_list : List
                consists of features of communities
                [[feature_of_community_1],[feature_of_community_2],...] 
        
        Returns
        -------
        distance[0][0] : float
                euclidean distance of two vectors in matrixes

        '''
        count = len(sg1_feature_list)
        print "lenght: %d"% count
        count = len(sg2_feature_list)
        print "lenght: %d"% count
        x = sg1_feature_list
        y = sg2_feature_list
        temp = 0.0
        for i in range(count):
                temp += (float(x[i] - y[i]))**2
        distance = temp ** 0.5  
        return distance
        
                

def obtain_score_between_features(long_features_list,short_features_list,long_short_index = None,method = euclidean_metric):
        '''
        calculate the similarity score between features by algritom specified method

        Parameters
        ----------

        sg1_feature_list , sg2_feature_list : List
                consists of features of communities
                [[feature_of_community_1],[feature_of_community_2],...] 
        
        method : pointer
                specified the method to calculate the distance between features of communities

        Returns
        -------
        
        scroe_list : List
                contain the distances between each pairs of features of communities 
                [
                        [community_1_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
                        [community_2_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
                        ....
                ]
        
        '''
        if long_short_index == None:
            big_feature_list = long_features_list
            score_list = [[] for i in range(len(big_feature_list))]
            small_feature_list = short_features_list

            for index in range(len(big_feature_list)):
                    s_feature = big_feature_list[index]
                    for d_feature in small_feature_list:
                            score = method(s_feature,d_feature)
                            score_list[index].append(score)
            return score_list
        else:
            long_index = [item[0] for item in long_short_index]
            short_index = [item[1] for item in long_short_index]
            score_list = [[] for i in range(len(long_index))]
            for li in range(len(long_index)):
                for si in range(len(short_index)):
                    score = method(long_features_list[long_index[li]],short_features_list[short_index[si]])
                    score_list[li].append(score)

        return score_list,long_index,short_index

def calculate_common_nodes_between_cmties(s_nodes_list,d_nodes_list):
        '''
        calculate the number of the nodes existed in both s_nodes_list and d_nodes_list

        Parameters
        ----------

        s_nodes_list , d_nodes_list : List
                consists of nodes index
        
        Returns
        -------

        common_nodes_rate : float , less than 1
                the partition of the common nodes in small nodes list

        common_nodes_list : List
                consists of nodes which appear in both 's_nodes_list' and 'd_nodes_list'
                
        '''
        if len(s_nodes_list) == 0 or len(d_nodes_list) == 0:
                return 0
        small_node_list = s_nodes_list if len(s_nodes_list) <= len(d_nodes_list) else d_nodes_list
        big_node_list = s_nodes_list if len(s_nodes_list) > len(d_nodes_list) else d_nodes_list
        common_nodes_list = []
        total_count = len(small_node_list) 
        for node in small_node_list:
                if node in big_node_list:
                        common_nodes_list.append(node)
        
        #print "source cmty: ",
        #print s_nodes_list[:8]
        #print "dest cmty: ",
        #print d_nodes_list[:8]
        common_count = len(common_nodes_list)
        #print "common nodes count: %d" % common_count
        #print "small length: %d" % len(small_node_list)
        common_nodes_rate = float(common_count)/total_count 
        #print "common rate: %.4f" % common_nodes_rate
        return common_nodes_rate, common_nodes_list

def obtain_most_similar_pairs_of_community(score_list,long_index,short_index):
    #new_score_list = np.array(score_list)
    #best_similar_list = np.where(new_score_list == np.min(new_score_list))
    #final_long_index = [long_index[item] for item in best_similar_list[0]]
    #final_short_index = [short_index[item] for item in best_similar_list[1]]
    #best_similar_value = []
    #for i in range(len(final_short_index)):
    #    best_similar_value.append(np.min(new_score_list))



    temp_matched_index = obtain_matched_cmty_index_with_best_matching(score_list)
    temp_matched_index = sorted(temp_matched_index,key=lambda x:x[2])

    if len(temp_matched_index) == 0:
        return [],[],[]

    #trust_matched_index_number = int(len(temp_matched_index) * 0.5) 
    trust_matched_index_number = 1 
    final_long_index = []
    final_short_index = []
    best_similar_value = []
    if trust_matched_index_number == 0:
        final_long_index.append(long_index[temp_matched_index[0][0]])
        final_short_index.append(short_index[temp_matched_index[0][1]])
        best_similar_value.append(temp_matched_index[0][2])
        return final_long_index,final_short_index,best_similar_value

    for item in temp_matched_index:
        final_long_index.append(long_index[item[0]])
        final_short_index.append(short_index[item[1]])
        best_similar_value.append(item[2])
        trust_matched_index_number -= 1
        if trust_matched_index_number == 0:
            break

    print "##############################################"
    print "trust matched index of communities:"
    for i in range(len(final_long_index)):
        print "%d-%d-%f\t"%(final_long_index[i],final_short_index[i],best_similar_value[i]),
    print "\n"
    print "##############################################"
    return final_long_index,final_short_index,best_similar_value

def obtain_first_third_of_closest_degree_pairs_communities(long_cmty_features,short_cmty_features,has_matched_pairs_index,main_feature_index):
    long_short_index = []
    has_matched_long_index = [item[0] for item in has_matched_pairs_index]
    has_matched_short_index = [item[1] for item in has_matched_pairs_index]
    long_cmty_index_nodes = [[index,long_cmty_features[index][main_feature_index]]for index in range(len(long_cmty_features)) if index not in has_matched_long_index]
    short_cmty_index_nodes = [[index,short_cmty_features[index][main_feature_index]]for index in range(len(short_cmty_features)) if index not in has_matched_short_index]

    long_cmty_index_nodes = sorted(long_cmty_index_nodes,key=lambda x:x[1],reverse = True)
    short_cmty_index_nodes = sorted(short_cmty_index_nodes,key=lambda x:x[1],reverse = True)
    length_first_thrid_pairs = math.ceil(len(short_cmty_index_nodes) * 0.5)
    
    for i in range(len(short_cmty_index_nodes)):
        long_short_index.append([long_cmty_index_nodes[i][0],short_cmty_index_nodes[i][0]])
        length_first_thrid_pairs -= 1
        if length_first_thrid_pairs <= 0:
            break
    print "#################################"
    print "candidate index of communities"
    print long_short_index
    print "#################################"
    return long_short_index

def obtain_shortest_path_between_nodes(long_G,source_nodes_list,dest_nodes_list):
    #print "start to compute the shortes path length...."
    shortest_path_mean_length = []
    for s in source_nodes_list:
        s_pathes_list = []
        for d in dest_nodes_list:
            s_pathes = nx.shortest_path_length(long_G,s,d)
            s_pathes_list.append(s_pathes)
        mean = np.array(s_pathes_list).mean()
        shortest_path_mean_length.append(mean)
    #print "shortest path mean length:"
    #print shortest_path_mean_length
    #print "start to compute the shortes path length...over!"
    return shortest_path_mean_length
        
def update_features_with_matched_communities(long_G,short_G,long_G_edges,short_G_edges,long_top_ten_nodes,short_top_ten_nodes,long_features_list,short_features_list,matched_index,lastest_matched_index):
    '''
    basied the matched pairs of communities as the seed, adding the mean of shotest path lengthes of the top ten nodes sorted by their degree
    '''
    has_matched_long_index = [item[0] for item in matched_index]
    has_matched_short_index = [item[1] for item in matched_index]

    lastest_matched_long_index = [item[0] for item in lastest_matched_index]
    lastest_matched_short_index = [item[1] for item in lastest_matched_index]
    unmatched_long_index = [i for i in range(len(long_top_ten_nodes)) if i not in has_matched_long_index]
    unmatched_short_index = [i for i in range(len(short_top_ten_nodes)) if i not in has_matched_short_index]

    sum_shortest_path_list = []  
    #mean_shortest_path_list = []
    #median_shortest_path_list = []
    for i in unmatched_long_index:
        for j in lastest_matched_long_index:
            #edges_count = obtain_edges_between_nodes(long_G_edges,long_top_ten_nodes[i],long_top_ten_nodes[j])
            #sum_shortest_path_list.append(edges_count)

            shortest_path_mean_length = obtain_shortest_path_between_nodes(long_G,long_top_ten_nodes[i],long_top_ten_nodes[j])
            sum_shortest_path_mean_length = sum(shortest_path_mean_length)
            #mean = np.array(shortest_path_mean_length).mean()
            sum_shortest_path_list.append(sum_shortest_path_mean_length)
            #mean_shortest_path_list.append(mean)
            #median = obtain_midian_list(shortest_path_mean_length)
            #median_shortest_path_list.append(median)
            #print "%d -> %d:"%(j,i)
            #print shortest_path_mean_length 
            #long_features_list[i].append(sum_shortest_path_mean_length)
            #long_features_list[i].append(mean)

    #normal the new feature
    max_sum_value = max(sum_shortest_path_list)  
    min_sum_value = min(sum_shortest_path_list)  
    sum_temp = max_sum_value - min_sum_value
    #max_mean_value = max(mean_shortest_path_list)
    #min_mean_value = min(mean_shortest_path_list)
    #mean_temp = max_mean_value - min_mean_value

    #max_median_value = max(median_shortest_path_list)  
    #min_median_value = min(median_shortest_path_list)  
    #median_temp = max_median_value - min_median_value
    for i in range(len(unmatched_long_index)):
        if sum_temp == 0:
            long_features_list[unmatched_long_index[i]].append(float(max_sum_value - sum_shortest_path_list[i]))
        else:
            long_features_list[unmatched_long_index[i]].append(float(max_sum_value - sum_shortest_path_list[i])/ sum_temp)

        #if mean_temp == 0:
        #    long_features_list[unmatched_long_index[i]].append(float(max_mean_value - mean_shortest_path_list[i]))
        #else:
        #    long_features_list[unmatched_long_index[i]].append(float(max_mean_value - mean_shortest_path_list[i])/ mean_temp)
        #if median_temp == 0:
        #    long_features_list[unmatched_long_index[i]].append(float(max_median_value - median_shortest_path_list[i]))
        #else:
        #    long_features_list[unmatched_long_index[i]].append(float(max_median_value - median_shortest_path_list[i])/ median_temp)


    sum_shortest_path_list = []  
    mean_shortest_path_list = []  
    median_shortest_path_list = []
    for i in unmatched_short_index:
        for j in lastest_matched_short_index:
            #edges_count = obtain_edges_between_nodes(short_G_edges,short_top_ten_nodes[i],short_top_ten_nodes[j])
            #sum_shortest_path_list.append(edges_count)
            shortest_path_mean_length = obtain_shortest_path_between_nodes(short_G,short_top_ten_nodes[i],short_top_ten_nodes[j])
            sum_shortest_path_mean_length = sum(shortest_path_mean_length)
            #mean = np.array(shortest_path_mean_length).mean()

            sum_shortest_path_list.append(sum_shortest_path_mean_length)
            #mean_shortest_path_list.append(mean)
            #median = obtain_midian_list(shortest_path_mean_length)
            #median_shortest_path_list.append(median)
            #print "%d -> %d:"%(j,i)
            #print shortest_path_mean_length 
            #short_features_list[i].append(edges_count)
    #normal the new feature
    max_sum_value = max(sum_shortest_path_list)  
    min_sum_value = min(sum_shortest_path_list)  
    sum_temp = max_sum_value - min_sum_value
    #max_mean_value = max(mean_shortest_path_list)
    #min_mean_value = min(mean_shortest_path_list)
    #mean_temp = max_mean_value - min_mean_value
    #max_median_value = max(median_shortest_path_list)  
    #min_median_value = min(median_shortest_path_list)  
    #median_temp = max_median_value - min_median_value
    for i in range(len(unmatched_short_index)):
        if sum_temp == 0:
            short_features_list[unmatched_short_index[i]].append(float(max_sum_value - sum_shortest_path_list[i]))
        else:
            short_features_list[unmatched_short_index[i]].append(float(max_sum_value - sum_shortest_path_list[i])/ sum_temp)
        #if mean_temp == 0:
        #    short_features_list[unmatched_short_index[i]].append(float(max_mean_value - mean_shortest_path_list[i]))
        #else:
        #    short_features_list[unmatched_short_index[i]].append(float(max_mean_value - mean_shortest_path_list[i])/ mean_temp)
        #if median_temp == 0:
        #    short_features_list[unmatched_short_index[i]].append(float(max_median_value - median_shortest_path_list[i]))
        #else:
        #    short_features_list[unmatched_short_index[i]].append(float(max_median_value - median_shortest_path_list[i])/ median_temp)
    print "after feature update"
    #print long_features_list
    #print short_features_list
    return 0

def obtain_edges_between_nodes(edges_list,source_nodes,dest_nodes):
    count = 0
    print "obtain edges between nodes..."
    for i in source_nodes:
        for j in dest_nodes:
            if [i,j] in edges_list:
                count += 1
                edges_list.remove([i,j])
            if [j,i] in edges_list:
                count += 1
                edges_list.remove([j,i])
    print "obtain edges between nodes...over!"
    return count

def show_real_matched_community_pairs(long_cmty_list,short_cmty_list):
    print "the real matched communities pairs"
    count = 0
    real_matched_index = []
    for s in range(len(long_cmty_list)):
        for d in range(len(short_cmty_list)):
            overlap,common_nodes_list = calculate_common_nodes_between_cmties(long_cmty_list[s],short_cmty_list[d]) 
            if overlap >= 0.5:
                print "%d <--> %d :common: %d  overlap: %f\t"%(s,d,len(common_nodes_list),overlap),
                real_matched_index.append([s,d,overlap])
                count += 1
    print "\nreal number of matched communities: %d"%count
    return count,real_matched_index
    return 0 

def obtain_main_feature(long_cmty_features,short_cmty_features,matched_index):
    '''
    return the index of feature that has the strong stdand error
    '''
    basic_features = [ "outdegree", "nodes", "edges","mean degree","midian_degree", "top ten nodes degree", "density_community"]
    has_matched_long_index = [item[0] for item in matched_index]
    has_matched_short_index = [item[1] for item in matched_index]
    umatched_long_cmty_features = [long_cmty_features[i] for i in range(len(long_cmty_features)) if i not in has_matched_long_index] 
    umatched_short_cmty_features = [short_cmty_features[i] for i in range(len(short_cmty_features)) if i not in has_matched_short_index] 
    group_long_features = []
    group_short_features = []
    index_long_features = range(len(long_cmty_features[0]))
    index_short_features = range(len(short_cmty_features[0]))
    for i in index_long_features:
        temp = [item[i] for item in umatched_long_cmty_features]
        group_long_features.append(temp)

    for i in index_short_features:
        temp = [item[i] for item in umatched_short_cmty_features]
        group_short_features.append(temp)

    long_stderr_features = [[i,np.array(group_long_features[i]).std()] for i in range(len(group_long_features))]
    long_stderr_features = sorted(long_stderr_features,key=lambda x:x[1],reverse=True)
    long_max_stderr = long_stderr_features[0][0]
    long_index = long_stderr_features[0][0] 
    short_stderr_features = [[i,np.array(group_short_features[i]).std()] for i in range(len(group_short_features))]
    short_stderr_features = sorted(short_stderr_features,key=lambda x:x[1],reverse=True)
    short_max_stderr = short_stderr_features[0][0]
    short_index = short_stderr_features[0][0] 

    index = short_index if short_max_stderr >= long_max_stderr else long_index

    if index < len(basic_features):
        print "The main features: %s" % basic_features[index]
    else:
        print "The main features: the other features"
    return index

def eavalute_accuracy_by_iteration_join_feature(long_G,short_G,long_G_edges,short_G_edges,long_cmty_list,short_cmty_list,low_threshold = 50,upper_threshold = 1000,method = euclidean_metric,detect_method = cnm.community_best_partition_with_limit):
    # step 1: initialize the basic feature list of communities
    long_cmty_features,long_cmty_features_nonomal,long_top_ten_nodes = obtain_cmty_feature_array(long_G,long_cmty_list,low_threshold,upper_threshold,obtain_basic_feature_of_cmty)
    short_cmty_features,short_cmty_features_nonormal,short_top_ten_nodes = obtain_cmty_feature_array(short_G,short_cmty_list,low_threshold,upper_threshold,obtain_basic_feature_of_cmty)

    overlap_list = []
    accuracy_rate = 0
    matched_index = []
    matched_count = 0
    while len(matched_index) < len(short_cmty_list):
        lastest_matchde_index = []
        print "%d pairs communities has matched!!" % len(matched_index)
        print matched_index 
        # step 2: get the first third of closest matched pairs of communities
        main_feature_index = obtain_main_feature(long_cmty_features_nonomal,short_cmty_features_nonormal,matched_index)
        #main_feature_index = 1
        long_short_index = obtain_first_third_of_closest_degree_pairs_communities(long_cmty_features_nonomal,short_cmty_features_nonormal,matched_index,main_feature_index)
        # step 3: obtain the similar score between the first third matched pairs of communities

        score_list,long_index,short_index = obtain_score_between_features(long_cmty_features,short_cmty_features,long_short_index,method = method)
        # step 4: choose the first pairs of community as the seed matched pairs
        long_matched_cmty_index,short_matched_cmty_index,similar_value = obtain_most_similar_pairs_of_community(score_list,long_index,short_index) 
        if long_matched_cmty_index == []:
            break
        for i in range(len(long_matched_cmty_index)):
            overlap,common_nodes_list = calculate_common_nodes_between_cmties(long_cmty_list[long_matched_cmty_index[i]],short_cmty_list[short_matched_cmty_index[i]]) 
            matched_index.append([long_matched_cmty_index[i],short_matched_cmty_index[i],len(common_nodes_list),overlap])
            lastest_matchde_index.append([long_matched_cmty_index[i],short_matched_cmty_index[i]])
            overlap_list.append(overlap)
            #print "matched index: %d - %d" % (long_matched_cmty_index[i],short_matched_cmty_index[i])
            #print "first matched community overlap: %.3f" % overlap
            #print "overlap length: %d"%len(common_nodes_list)
            if(overlap >= 0.50):
                matched_count += 1
                print "matched communities : %d"% matched_count
        # step 5: add a new feature into the unmatched pairs of communities
        update_features_with_matched_communities(long_G,short_G,long_G_edges,short_G_edges,long_top_ten_nodes,short_top_ten_nodes,long_cmty_features,short_cmty_features,matched_index,lastest_matchde_index)
        
        # step 6: go to step 2 until all communities has matched!
    accuracy_rate = float(matched_count) / len(matched_index)
    return accuracy_rate,overlap_list,matched_index,long_cmty_features_nonomal,short_cmty_features_nonormal

def eavalute_accuracy_by_feature_degree_distribution(long_G,short_G,long_cmty_list,short_cmty_list,low_threshold = 50,upper_threshold = 1000,method = euclidean_metric,detect_method = cnm.community_best_partition_with_limit):

    '''
    Return the matched pairs of communities from the 'G1' and 'G2' and calculate the matching accuracy rate between the communities detected by the method specified by 'detect_methond'    

    Parameters
    ----------
    G1,G2 : networkx graph
            original networkx graph

    low_threshold : int
            the minimum number of nodes in a community

    upper_threshold : int
            the minimum number of nodes in a community
    
    method : pointer
            point the  function of calculating distance bewteen communities' features

    detect_method : pointer
            point the  function of detecting tht communities in graphes 

    Returns
    -------

    rate : float    
        the accuracy rate of matched pairs of communities

    left_Graph,right_Graph : networkx graph
        specified which graph the left communities or right communities in 'matched_index'      belong to 
                    example: matched_index = [[1,3],[5,6],...]. the community of index 1 is in left_Graph and the community of index 3 is in right_Graph 

            left_cmty_list,right_cmty_list : List
                    consist of the communities in left Graph and right Graph
                    [[community_1],[community_2],....]
            
            matched_index : List
                    consists of matched index of communities between the left Graph and right Graph
                    [[1,3],[4,5],...]

            scroe_list : List
                    contain the distances between each pairs of features of communities 
                    [
                            [community_1_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
                            [community_2_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
                            ....
                    ]
            
    '''
    # The type of SG1 and SG2 is the type Snap needs 
    # SG1_ret_list and SG2_ret_list is the result of the detection of graph

    long_cmty_features,long_cmty_features_nonomal,long_top_ten_nodes = obtain_cmty_feature_array(long_G,long_cmty_list,low_threshold,upper_threshold,feature_method = obtain_feature_of_cmty_with_degree_distribution)
    short_cmty_features,short_cmty_features_nonormal,short_top_ten_nodes = obtain_cmty_feature_array(short_G,short_cmty_list,low_threshold,upper_threshold,feature_method = obtain_feature_of_cmty_with_degree_distribution)
    print "long : %d" % len(long_cmty_features)
    print "short : %d" % len(short_cmty_features)

    score_list = obtain_score_between_features(long_cmty_features,short_cmty_features,method = method)
    
    accuracy_rate,overlap_list,matched_index = calculate_accuracy_rate_by_feature(long_G,long_cmty_list,short_G,short_cmty_list,score_list,long_cmty_features,short_cmty_features,throd_value = 0.7)

    return accuracy_rate,overlap_list,matched_index,long_cmty_features_nonomal,short_cmty_features_nonormal

def obtain_matched_cmty_index_with_best_matching(score_list):
    matched_index = []
    loop_count = len(score_list)  
    small_cmty_count = len(score_list[0])
    have_matched_big_new_cmty = {} 
    for i in range(0,loop_count):
        #print "**************************************************************"
        #obtain the similarity list of ith community with all of the other community
        similarity_list =[(index,score_list[i][index]) for index in range(0,small_cmty_count)]
        #sort the similiarity of ith community such that obtain the most similar one
        similarity_list = sorted(similarity_list,key=lambda x:x[1])
        #print "%d pairs have been matched!!" % len(have_matched_big_new_cmty)           
        if len(have_matched_big_new_cmty) == small_cmty_count:
                break
        while len(similarity_list) > 0:
                #obtain the similiarity list of the best one similiaried with the ith cmty from big community list
                best_score = similarity_list[0][1]
                C_index = similarity_list[0][0]
                #print "best score: %.5f" % best_score
                #print "C index: %d"% C_index
        
                similarity_list.pop(0)
                #obtain the community which is most similar to the community specified by C_index
                dest_similarity_list = []       
                dest_cmty_index = 0
                for item in score_list:
                        #if dest_cmty_index had matched before
                        if dest_cmty_index in have_matched_big_new_cmty.keys() and have_matched_big_new_cmty[dest_cmty_index] != C_index:
                                dest_cmty_index += 1
                                continue
                        dest_similarity_list.append(item[C_index])
                        dest_cmty_index += 1
        
                dest_similarity_list = sorted(dest_similarity_list)
                #print dest_similarity_list
                if best_score > dest_similarity_list[0]:
                        continue;
                break
        #no matched if length of the similiarity list is zero,guasee the firsted matched community is the best one
        if len(similarity_list) == 0:
                continue
        #print "best candidate: %d" % C_index
        #add the index of big cmty 
        have_matched_big_new_cmty[i] = C_index
        matched_index.append([i,C_index,score_list[i][C_index]])
    return matched_index 


def calculate_accuracy_rate_by_feature(long_G,long_cmty_list,short_G,short_cmty_list,score_list,long_features_list,short_features_list,throd_value = 0.75):
        '''     
        calculate the accuracy rate of matching communities between graphes

        Parameters
        ----------

        SG1 ,SG2 : networkx graph
                original netwrokx graph
        SG1_new_cmty , SG2_new_cmty : List
                the communities list of the SG1 graph and SG2 graph
        
        scroe_list : List
                contain the distances between each pairs of features of communities 
                [
                        [community_1_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
                        [community_2_of_left_Graph,similiarity_1_with_community_in_right_graph,similiarity_2_with_community_in_right_graph,similiarity_3_with_community_in_right_graph,...]
                        ....
                ]
        
        SG1_feature,SG2_feature : List
                consists of features of communities
                [[feature_of_community_1],[feature_of_community_2],...] 
        
        throd_value : float
                the threshold of accuracy rate of the nodes between the matched communities 
                
        Returns
        -------

        accuracy_rate : float
                the accuracy rate of the matched communities

        big_G : networkx graph
                the number of communities in this graph is greater

        samll_G : networkx graph
                the number of communities in this graph is smaller 

        big_new_cmty : List
                the communities list in 'big_G'

        small_new_cmty : List
                the communities list in 'small_G'
        
        matched_index : List
                contains the pairs index of communities. 
                The left index of pairs is the community from the big_G graph 
                The right index of pairs is the community from the small_G graph 

        return the result of the matched communities index
        '''
        matched_count = 0
        unmatched_count = 0
        matched_index = []

        big_new_cmty,big_G = long_cmty_list,long_G 
        small_new_cmty,small_G = short_cmty_list,short_G 
        big_feature = long_features_list 
        small_feature = short_features_list 

        overlap_list = []

        loop_count = len(big_new_cmty)  
        small_cmty_count = len(small_new_cmty)
        have_matched_big_new_cmty = {} 
        for i in range(0,loop_count):
                print "**************************************************************"
                #obtain the similarity list of ith community with all of the other community
                similarity_list =[(index,score_list[i][index]) for index in range(0,small_cmty_count)]
                #sort the similiarity of ith community such that obtain the most similar one
                similarity_list = sorted(similarity_list,key=lambda x:x[1])
                
                #print "%d pairs have been matched!!" % len(have_matched_big_new_cmty)           
                if len(have_matched_big_new_cmty) == small_cmty_count:
                        break

                print "cmty: %d" % i
                print "similarity list: ",
                print similarity_list

                while len(similarity_list) > 0:
                        #obtain the similiarity list of the best one similiaried with the ith cmty from big community list
                        best_score = similarity_list[0][1]
                        C_index = similarity_list[0][0]
                        print "best score: %.5f" % best_score
                        print "C index: %d"% C_index

                        similarity_list.pop(0)
                        #obtain the community which is most similar to the community specified by C_index
                        dest_similarity_list = []       
                        dest_cmty_index = 0
                        for item in score_list:
                                #if dest_cmty_index had matched before
                                if dest_cmty_index in have_matched_big_new_cmty.keys() and have_matched_big_new_cmty[dest_cmty_index] != C_index:
                                        dest_cmty_index += 1
                                        continue
                                dest_similarity_list.append(item[C_index])
                                dest_cmty_index += 1

                        dest_similarity_list = sorted(dest_similarity_list)
                        #print dest_similarity_list
                        if best_score > dest_similarity_list[0]:
                                continue;
                        break
                #no matched if length of the similiarity list is zero,guasee the firsted matched community is the best one
                if len(similarity_list) == 0:
                        unmatched_count += 1
                        continue
                print "best candidate: %d" % C_index
                temp_rate,common_nodes_list = calculate_common_nodes_between_cmties(big_new_cmty[i],small_new_cmty[C_index]) 

                #add the index of big cmty 
                have_matched_big_new_cmty[i] = C_index
                matched_index.append([i,C_index,len(common_nodes_list),temp_rate])
                print "overlap rate: %.4f" % temp_rate
                overlap_list.append(temp_rate)
                if temp_rate >= throd_value:
                        print "mapping successful!"
                        matched_count += 1
                else:
                        print "mapping failed"
                        unmatched_count += 1
                print "matched count: %d" % matched_count
                print "unmatched count: %d" % unmatched_count
        
        accuracy_rate = float(matched_count)/len(matched_index)
        print "total count: %d" % len(matched_index) 
        print "accuracy rate: %.5f" % accuracy_rate
        return accuracy_rate,overlap_list,matched_index


def deepwalk_map_prob_maxtrix(small_G,long_G,dimensions):
        '''
        Return the matched nodes pairs between the small_G graph and long_G graph       

        Parameters
        ----------

        small_G : networkx graph
                the number of nodes in this graph is smaller

        long_G : networkx graph
                the number of nodes in this graph is greater 
        
        dimensions : int
                specified the dimensions of each node' feature in graph 

        Returns
        -------

        matches : List 
                consist of matched nodes pairs. 
                The node specified by left index in pair is from the small_G Graph      
                The node specified by right index in pair is from the small_G Graph  

                [[1,2],[32,3],...]: node 1 is from small_G graph, node 2 is from long_G graph
        
        '''
        print "detect seed list with deepwalk....."
        small_node,long_node,P = dm.map_prob_maxtrix(small_G,long_G,dimensions=dimensions)
        count = 0
        long_loop_count = long_G.order()
        
        matches = []

        for i in range(len(small_node)):
                #obtain ith row
                similarity_list =[(index,P[i][index]) for index in range(long_loop_count)]
                similarity_list = sorted(similarity_list,key=lambda x:x[1],reverse = True)
                while len(similarity_list) > 0:
                #record the matched nodes obtained by deepwalk  
                        #obtain the similiarity list of the best one similiaried with the ith cmty from big community list
                        best_score = similarity_list[0][1]
                        C_index = similarity_list[0][0]

                        similarity_list.pop(0)

                        dest_similarity_list = []       
                        for item in P:
                                dest_similarity_list.append(item[C_index])

                        dest_similarity_list = sorted(dest_similarity_list,reverse=True)
                        if best_score < dest_similarity_list[0]:
                                continue;
                        break

                if len(similarity_list) == 0:
                        #not finding the best matched node
                        continue
                
                #record the nodes pairs which is same one node judeged by algrithm
                if best_score >= 1.0:
                        matches.append((small_node[i],long_node[C_index]))

        return matches

        
def obtain_accuracy_rate_in_matched_cmty(left_graph,left_cmty_list,right_graph,right_cmty_list,matched_cmty_index_pairs,dimensions=65,seed_matching_method = dm.bipartite_matching,z_score_threshold = 2.0):
        '''
                There are 5 stages to match the nodes in different graph:
                1. Detection the community for the graphes
                2. Matching the communities by using their features 
                3. Pre-process stage : apply the nodes matching method, deepwalk&CPD or bipartitie_matching to the subgraphes generated wiht the matched communities pairs and obtain the  matched nodes pairs for the communities pairs
                4. Edges-credibility stage : With the matched nodes pairs obtained in stage 3 as the input to this stage, using the edges credibility algritom to detect the real seed nodes pairs as the output of this stage   
                5. Refine stage : With the real seed nodes pairs obtained in stage 4, applying the propagation algritom to match the rest nodes of the different subgraphes.
        
                Under the 5 stages, this function will return the matched nodes pairs obtained in each stages except the first stage.

                Parameters
                ----------

                left_graph,right_graph : networkx graph
                        specified which graph the left communities or right communities in 'matched_cmty_index_pairs'  belong to 
                        example: matched_cmty_index_pairs = [[1,3],[5,6],...]. the community of index 1 is in left_graph and the community of index 3 is in right_graph 

                left_cmty_list,right_cmty_list : List
                        consist of the communities in left Graph and right Graph

                dimensions : int
                        specified the dimensions of each nodes' features in graph using by deepwalk&CPD algritom 

                seed_matching_method : pointer
                        specified the matching nodes pairs method
                        
                Returns
                -------

                matched_nodes_number: list
                        Four elments in each item of this list is list and each elment consists of 8 numbers :
                         1. the number of all nodes in the      matched communities and the number of same nodes bewteen matched communities
                         2. the number of all nodes obtained by deepwalk and the number of the same nodes
                         3. the number of the all seed nodes obtained by edge_consistency_method and the number of the real seed nodes
                         4. the number of all nodes obtained by refine stage and the number of the real same nodes pairs

                         [[100,90,80,75,75,50,100,90],[100,90,80,75,75,50,100,90],.....]

                

                pre_process_seed_rate : List,elment type :float
                        the ratio of the number of same nodes pairs to the number of the all  nodes pairs for the each matched communities pair in pre_process stage. 
                
                pre_process_seed_nodes_list : List ,elment type : List

                        consists of nodes pairs obtained in pre_process stage for each communities pair.
                        form as following:
                        [
                                [[1,4],[2,3],...],
                                [[90,48],[70,40],....],
                                ...
                        ]
                                
                refine_nodes_rate : List ,elment type :float
                        the ratio of the number of same nodes pairs to the number of the all  nodes pairs for the each matched communities pair in refine stage. 

                refine_nodes_matches_list : List, elment type: List
                        consists of nodes pairs obtained in refine stage for each communities pair.
                        form as following:
                        [
                                [[1,4],[2,3],...],
                                [[90,48],[70,40],....],
                                ...
                        ]
                
                z_score_list : List, elment type : float
                        consists of the z_score for each communities pairs

        '''
        #left element of items in seeds_of_global_graph is the node from left graph
        #right element of items in seeds_of_global_graph is the node from right graph
        seeds_of_global_graph = []
        matched_nodes_number = []
        pre_process_seed_rate = []
        pre_process_seed_nodes_list = []
        refine_nodes_rate = []
        refine_nodes_matches_list = []
        z_score_list = []
        for item in matched_cmty_index_pairs:
                left_index = item[0]
                right_index = item[1]
                common_nodes = item[2]
                original_size = len(left_cmty_list[left_index]) if len(left_cmty_list[left_index]) <= len(right_cmty_list[right_index]) else len(right_cmty_list[right_index])

                print "%i -> %i common nodes: %d" % (left_index,right_index,common_nodes)
                #if common_nodes == 0:
                #       continue
                #tansfrom the community to the graph
                G1 = left_graph.subgraph(left_cmty_list[left_index])
                G2 = right_graph.subgraph(right_cmty_list[right_index])

                #judge the small number of nodes of graph
                small_original_graph,small_G = (left_graph,G1) if G1.order() <= G2.order() else (right_graph,G2)
                long_original_graph,long_G = (left_graph,G1) if G1.order() > G2.order() else (right_graph,G2)

                
                #mapping the nodes between the communities
                #pre-process stage
                print "pre-process stage is begining...."

                if seed_matching_method == dm.bipartite_matching:
                        matches = seed_matching_method(small_G,long_G)
                if seed_matching_method == deepwalk_map_prob_maxtrix:
                        matches = seed_matching_method(small_G,long_G,dimensions=dimensions)

                deepwalk_common_nodes_list = [] 
                deepwalk_matched_nodes_size = len(matches)
                deepwalk_real_seed_count = 0
                for item in matches:
                        if item[0] == item[1]:
                                deepwalk_real_seed_count += 1
                                deepwalk_common_nodes_list.append(item[0])
                if deepwalk_real_seed_count == 0:
                        print "no one is right in the matched obtained by deepwalk"
                #pre-process stage: seed nodes detection
                pre_process_list,pre_process_count,pre_process_rate,z_score = obtain_seed_with_edges_credibility(matches,small_G,long_G,deepwalk_common_nodes_list,left_index,right_index)

                pre_process_seed_rate.append(pre_process_rate)
                pre_process_seed_nodes_list.append(pre_process_list)
                z_score_list.append(z_score)

                print "pre-process stage accuracy rate: %.2f" % pre_process_rate
                print "%d : %d" % (len(pre_process_list),pre_process_count) 
                if len(pre_process_list) != 0:
                        print "temp rate: %.2f" % (pre_process_count*1.0 / len(pre_process_list))
                print "pre-process and edge credibility stage has completed!"
                
                #collect the seeds into the global list.
                #the left element of item in seeds_of_global_graph is the nodes from left graph
                if z_score > z_score_threshold:
                    if small_original_graph == left_graph:
                        for i in pre_process_list:
                            seeds_of_global_graph.append(i)
                    else:
                        for i in pre_process_list:
                            seeds_of_global_graph.append([i[1],i[0]])
                print "the %d seeds have collect!!" % len(seeds_of_global_graph)
                #refine stage: refine the remain nodes in the graph with seed list
                #print "refine stage start ...."
                #refine_match_nodes,refine_real_matched_nodes_count,refine_rate = match_propagation(pre_process_list,small_G,long_G)
                #refine_nodes_rate.append(refine_rate)
                #refine_nodes_matches_list.append(refine_match_nodes) 
                #print "refine stage accuracy rate: %.2f" % refine_rate
                #print "%d : %d" % (len(refine_match_nodes),refine_real_matched_nodes_count) 
                #print "refine stage has completed!"
                
                matched_nodes_number.append([original_size,common_nodes,deepwalk_matched_nodes_size,deepwalk_real_seed_count,len(pre_process_list),pre_process_count,0,0])
                #matched_nodes_number.append([original_size,common_nodes,deepwalk_matched_nodes_size,deepwalk_real_seed_count,len(pre_process_list),pre_process_count,len(refine_match_nodes),refine_real_matched_nodes_count])
        #propagation in global graph
        seeds_crebility_count = 0
        for item in seeds_of_global_graph:
                if item[0] == item[1]:
                        seeds_crebility_count += 1
        if len(seeds_of_global_graph) > 0:
            global_seeds_accuracy_rate = (float(seeds_crebility_count) /len(seeds_of_global_graph))
        else:
            global_seeds_accuracy_rate = 0
            refine_rate = 0
            return matched_nodes_number,pre_process_seed_rate,pre_process_seed_nodes_list,refine_nodes_rate,refine_nodes_matches_list,z_score_list,global_seeds_accuracy_rate,refine_rate
        print "all seeds accuary rage: %f" % global_seeds_accuracy_rate   

        refine_match_nodes,refine_real_matched_nodes_count,refine_rate = match_propagation(seeds_of_global_graph,left_graph,right_graph)
        print "propagation accuracy rate: %.2f" % refine_rate
        print "%d : %d" % (len(refine_match_nodes),refine_real_matched_nodes_count) 
        return matched_nodes_number,pre_process_seed_rate,pre_process_seed_nodes_list,refine_nodes_rate,refine_nodes_matches_list,z_score_list,global_seeds_accuracy_rate,refine_rate

        #return for cmty mapping 
        #return matched_nodes_number,pre_process_seed_rate,pre_process_seed_nodes_list,refine_nodes_rate,refine_nodes_matches_list,z_score_list,0,0

def draw_subgraph_after_deepwalk(small_G,long_G,matched_nodes_pairs,deepwalk_common_nodes_list):
        '''
        Draw the subgraph after pre_process stage----deepwalk&CPD.

        '''
        partitions = [] 
        partitions.append([node for node in matched_nodes_pairs if node not in deepwalk_common_nodes_list])
        partitions.append([matched_nodes_pairs[node] for node in matched_nodes_pairs if matched_nodes_pairs[node] not in deepwalk_common_nodes_list])
        sub_nodes_list = partitions[0] + partitions[1]
        subgraph_small = small_G.subgraph(sub_nodes_list)
        draw_partitions(subgraph_small,partitions)
        partitions_matched = []
        partitions_matched.append([node for node in matched_nodes_pairs if node not in deepwalk_common_nodes_list])
        partitions_matched.append([matched_nodes_pairs[node] for node in matched_nodes_pairs if matched_nodes_pairs[node] not in deepwalk_common_nodes_list])
        sub_nodes_list_matched = partitions_matched[0] + partitions_matched[1]
        subgraph_big = long_G.subgraph(sub_nodes_list_matched)
        draw_partitions(subgraph_big,partitions_matched)
        return

def plot_z_score(z_score_list):
        '''
        Plot the consistency sequence

        Parameters
        ----------

        z_score_list : List, elment type : float
                consists of the z_score for each communities pairs

        Returns 
        -------

                No returns
        '''
        fig, ax = plt.subplots()
        ax.plot(z_score_list, '.-')
        ax.set_xlabel('Community Index',fontsize = 17)
        ax.set_title('Each Matching Z_Score',fontsize=18)
        ax.set_ylabel('Z_score',fontsize = 17)
        plt.tight_layout()
        plt.show()

def obtain_community_detection_in_graph(G1,G2,low_threshold,upper_threshold,detect_method = cnm.community_best_partition_with_limit):
        # The type of SG1 and SG2 is the type Snap needs 
        # SG1_ret_list and SG2_ret_list is the result of the detection of graph
        G1_cmty_list,G2_cmty_list = community_detect_graph(G1,G2,detect_method = detect_method,limit_ceil_cmty=upper_threshold)
        print "G1 original length: %d"%len(G1_cmty_list)
        print "G2 original length: %d"%len(G1_cmty_list)

        G1_new_cmty_list = []
        G2_new_cmty_list = []
        for cmty in G1_cmty_list:
                if len(cmty) > low_threshold:
                        G1_new_cmty_list.append(cmty)
        for cmty in G2_cmty_list:
                if len(cmty) > low_threshold:
                        G2_new_cmty_list.append(cmty)
        return G1_new_cmty_list,G2_new_cmty_list

def save_graph_community(filename,remarks,sample,low_threshold,upper_threshold):
    # save the grpah and community
    G = load_graph_from_file(filename,comments = '#',delimiter = ' ')
    print "dataset has loaded successful!"
    G1,G2 = bi_sample_graph(G,sample)
    G1_community_list,G2_community_list = obtain_community_detection_in_graph(G1,G2,low_threshold,upper_threshold,detect_method = cnm.community_best_partition_with_limit)
    print "community detection...over!"
    print "save community detection..."
    #save the G1
    df = open(remarks+".txt","w")
    df.write("######################################################\n")
    df.write("#date: %s\n" % ti.strftime("%Y-%m-%d %H:%M:%S",ti.localtime(ti.time())))
    df.write("#dataset: %s\n" % (filename))
    df.write("#remarks: %s\n"%remarks)
    df.write("#sample: %.2f\n" % (sample))
    df.write("#low threshold: %d\n"%low_threshold)
    df.write("#upper threshold: %d\n"%upper_threshold)
    df.write("#G1 %d nodes and %d edges\n"%(G1.number_of_nodes(),G1.number_of_edges()))
    df.write("#G2 %d nodes and %d edges\n"%(G2.number_of_nodes(),G2.number_of_edges()))

    G1_all_edges = [e for e in G1.edges]
    df.write("#G1:")
    for item in G1_all_edges:
        df.write("%d %d-"%(item[0],item[1]))
    df.write("\n")
    G2_all_edges = [e for e in G2.edges]
    df.write("#G2:")
    for item in G2_all_edges:
        df.write("%d %d-"%(item[0],item[1]))
    df.write("\n")

    df.write("#G1 community number: %d\n"%len(G1_community_list))
    for item in G1_community_list:
        for i in item:
            df.write("%d "%i)
        df.write("\n")
    df.write("#G2 community number: %d\n"%len(G2_community_list))
    for item in G2_community_list:
        for i in item:
            df.write("%d "%i)
        df.write("\n")
    print "save community detection...over!"


def load_graph_with_community(filename):
    sf = open(filename)
    line = sf.readline()
    G1 = nx.Graph()
    G2 = nx.Graph()
    dataset = ""
    remarks = ""
    low_threshold = 0
    upper_threshold = 0
    sample = 0
    G1_edges = []
    G2_edges = []
    while "#G1:" not in line:
        if "#remarks: " in line:
            remarks = line.strip().split(' ')[-1]
        if "#dataset: " in line:
            dataset = line.strip().split(' ')[-1]
        if "#low threshold:" in line:
            low_threshold = int(line.strip().split('#low threshold: ')[-1])
        if "#upper threshold:" in line:
            upper_threshold = int(line.strip().split('#upper threshold: ')[-1])
        if "#sample:" in line:
            sample = float(line.strip().split('#sample: ')[-1])
        line = sf.readline()

    #load G1
    line = line.strip().split('#G1:')[-1]
    line = line.strip().split('-')[:-1]
    for item in line:
        item = item.split(' ')
        G1_edges.append([int(item[0]),int(item[1])])
        G1.add_edge(int(item[0]),int(item[1]))

    while "#G2:" not in line:
        line = sf.readline()
    #load G2
    line = line.strip().split('#G2:')[-1]
    line = line.strip().split('-')[:-1]
    for item in line:
        item = item.split(' ')
        G2_edges.append([int(item[0]),int(item[1])])
        G2.add_edge(int(item[0]),int(item[1]))

    while "#G1 community" not in line:
        line = sf.readline()
    length_G1_community = int(line.strip().split(" ")[-1])
    G1_community_list = []
    line = sf.readline()
    for i in range(length_G1_community):
        line = line.strip().split(" ")
        tmp = []
        for node in line:
            tmp.append(int(node))
        G1_community_list.append(tmp)
        line = sf.readline()

    while "#G2 community" not in line:
        line = sf.readline()
    length_G2_community = int(line.strip().split(" ")[-1])
    G2_community_list = []
    line = sf.readline()
    for i in range(length_G2_community):
        line = line.strip().split(" ")
        tmp = []
        for node in line:
            tmp.append(int(node))
        G2_community_list.append(tmp)
        line = sf.readline()
    return G1,G2,G1_edges,G2_edges,G1_community_list,G2_community_list,dataset,remarks,sample,low_threshold,upper_threshold


def main():
    #=============================================================================#
    #save the community
    #if len(sys.argv) < 6:
    #        print "usage: ./deepmatching_for_cmty.py [filename] [sample] [low_threshold] [upper_threshold] [remarks]"
    #        return -1
    #save_graph_community(sys.argv[1],sys.argv[5],float(sys.argv[2]),int(sys.argv[3]),int(sys.argv[4]))
    #=============================================================================#
    #=============================================================================#
    # load the community

    if len(sys.argv) < 3:
        print "usage: ./deepmatching_for_cmty.py [filename] [community mapping method]"
        return -1
    G1,G2,G1_edges,G2_edges,G1_community_list,G2_community_list,dataset,remarks,sample,low_threshold,upper_threshold = load_graph_with_community(sys.argv[1])
    print "dataset: %s" % dataset
    print "sample: %f" % sample
    print "low threshold: %d" % low_threshold
    print "upper threshold: %d" % upper_threshold
    
    print "G1: %d nodes and %d edges" %(G1.number_of_nodes(),G1.number_of_edges()) 
    print "G1 community length: %d" % len(G1_community_list)
    print "G2: %d nodes and %d edges" %(G2.number_of_nodes(),G2.number_of_edges()) 
    print "G2 community length: %d" % len(G2_community_list)
    print "load graph and community successfully!"
    long_cmty_list,long_G,long_G_edges = (G1_community_list,G1,G1_edges) if len(G1_community_list) >= len(G2_community_list) else (G2_community_list,G2,G2_edges)
    short_cmty_list,short_G,short_G_edges = (G1_community_list,G1,G1_edges) if len(G1_community_list) < len(G2_community_list) else (G2_community_list,G2,G2_edges)
    filename = "%s_%s_community_matching_result.txt"%(remarks.split('.')[0],sys.argv[2])
    df = open(filename,"w")
    df.write("########################################################################\n")
    df.write("#date: %s\n" % ti.strftime("%Y-%m-%d %H:%M:%S",ti.localtime(ti.time())))
    df.write("#dataset: %s\n" % (dataset))
    df.write("#sample: %.2f\n" % (sample))
    df.write("#similarity function: %s\n" % "euclidean_distance")
    df.write("#low thresholdof the community: %d\n" % low_threshold)
    df.write("#upper thresholdof the community: %d\n" % upper_threshold)
    df.flush()
    #show the actually matched index of communities
    true_matchde_count,true_matched_index = show_real_matched_community_pairs(long_cmty_list,short_cmty_list)
    df.write("#number of real matched communities: %d\n"%true_matchde_count)
    df.write("#real matched index:\n")
    for item in true_matched_index:
        df.write("%d %d %.5f"%(item[0],item[1],item[2]))
        df.write("-")
    df.write("\n")
    #community_accuracy_rate,overlap_list,matched_index,long_cmty_features,short_cmty_features = eavalute_accuracy_by_feature_degree_distribution(long_G,short_G,long_cmty_list,short_cmty_list,low_threshold = low_threshold,upper_threshold = upper_threshold)
    community_accuracy_rate,overlap_list,matched_index,long_cmty_features,short_cmty_features = eavalute_accuracy_by_iteration_join_feature(long_G,short_G,long_G_edges,short_G_edges,long_cmty_list,short_cmty_list,low_threshold = low_threshold,upper_threshold = upper_threshold)
    
    df.write("#matched index:")
    for item in matched_index:
        df.write("%d-%d-%d-%f "%(item[0],item[1],item[2],item[3]))
    df.write("\n")
    df.write("#basic features: outdegree nodes edges mean_degree_top_ten_nodes midian_Degree_top_ten density_community\n")
    for i in range(len(long_cmty_features)):
        df.write("long community index %d: \n"% i)
        for item in long_cmty_features[i]:
            df.write("%.5f "%item)
        df.write("\n")

    for i in range(len(short_cmty_features)):
        df.write("short community index %d: \n"% i)
        for item in short_cmty_features[i]:
            df.write("%.5f "%item)
        df.write("\n")
    print "community_accuracy_rate : %.4f" % community_accuracy_rate
    print "community_accuracy_rate : %.4f" % community_accuracy_rate
    print "overlap rate: "
    print overlap_list
    overlap_list = np.array(overlap_list)
    print "average overlap rate: %.3f" % (overlap_list.mean())
    df.write("#total number of communities: %d\n"%len(short_cmty_list))
    df.write("#total matched number of communities: %d\n"%len(matched_index))
    df.write("#community mapping accuracy rate: %.3f\n"%community_accuracy_rate);
    df.write("#average overlap rate: %.3f\n"%overlap_list.mean())
    df.write("#stderr overlap rate: %.3f\n"%overlap_list.std())
    df.write("########################################################################\n")
    df.flush()
    print "result will be recorded in %s"%filename
#
#        for crebility_threshold in [2.0,6.0,10.0]:
#            df.write("#z score threshold: %0.3f\n"%crebility_threshold)
#            print "crebility threshold : %f"%crebility_threshold
#            for i in range(repeated_count):
#                    #obtain the pairs of index of the matched communities
#                    community_accuracy_rate,overlap_list,matched_index = repeated_eavalute_accuracy_by_feature(long_G,short_G,low_threshold = low_threshold,upper_threshold = upper_threshold)
#                    matched_nummber_nodes,pre_process_seed_rate,pre_process_seed_nodes_list,refine_rate_list,refine_match_nodes_list,z_score_list,global_seeds_accuracy_rate,refine_rate = obtain_accuracy_rate_in_matched_cmty(left_graph,left_cmty_list,right_graph,right_cmty_list,matched_index,dimensions,z_score_threshold = crebility_threshold)
#                    if global_seeds_accuracy_rate == 0:
#                        i = i - 1;
#                        continue
#                    df.write("# %ith loop:\n"%i)
#                    print "current %d loop"%i
#                    #record the community mapping result
#                    df.write("#Communities mapped results[source cmty id,dest cmty id,common nodes number]: \n")
#                    for item in matched_index:
#                            df.write("%d-%d-%d-%0.4f "%(item[0],item[1],item[2],item[3]));
#                            df.flush()
#                    df.write("\n")
#                    df.write("#total community mapped number: %d\n"%(len(matched_index)))
#                    df.write("#community mapping accuracy: %0.4f\n"%(community_accuracy_rate));
#
#                     
#                    #record the small commnuity size
#                    df.write("#left graph cmty total number: %d\n"%len(left_cmty_list));
#                    df.write("#left graph cmty length distribution: \n")
#                    for index in left_cmty_list:
#                            df.write("%i\t" % (len(index)))
#                            df.flush()
#                    df.write("\n")
#
#                    df.write("#right graph cmty total number: %d\n"%len(right_cmty_list));
#                    df.write("#right graph cmty length distribution: \n")
#                    for index in right_cmty_list:
#                            df.write("%i\t" % (len(index)))
#                            df.flush()
#                    df.write("\n")
#
#                    df.write("#cmty similarity between the left and right graph: \n")
#                    for item in score_list:
#                            df.write("index %d: "% score_list.index(item))
#                            for score in item:
#                                    df.write("%f "% float(score))
#                            df.write("\n")
#                    df.write("#################################################\n")
#
#                    #record the features of the each community
#                    df.write("#features name list: \n")
#                    for item in features_name_list:
#                            df.write("%s\t"%item);
#                            df.flush()
#                    df.write("\n")
#
#                    df.write("#left graph cmty features: \n")
#                    cmty_index = 0
#
#                    for cmty_index in range(len(SG1_features_list)):
#                            df.write("#community %d: "% cmty_index)
#                            for item in SG1_features_list[cmty_index]:
#                                    df.write("%.5f\t" % item)
#                                    df.flush()
#                            df.write("\n")
#                    df.write("#right graph cmty features: \n")
#                    cmty_index = 0
#                    for cmty_index in range(len(SG2_features_list)):
#                            df.write("#community %d: "% cmty_index)
#                            for item in SG2_features_list[cmty_index]:
#                                    df.write("%.5f\t" % item)
#                                    df.flush()
#                            df.write("\n")
#
#                    ## the dimensions of feature of the nodes obtained by deepwalk 
#            
#                    df.write("#dimensions: %d\n" % dimensions)
#                    df.write("#deepwalk results[cmty-deepwalk-seed]: ")
#                    for item in matched_nummber_nodes:
#                            df.write("%d-%d-%d-%d-%d-%d-%d-%d " % (item[0],item[1],item[2],item[3],item[4],item[5],item[6],item[7]))
#                    df.flush()
#                    df.write('\n')
#                    df.flush()
#                    #finding the seed nodes and calculating the accuracy rate
#                    df.write("#pre-process stage seed accuracy rate: ")
#                    for item in pre_process_seed_rate:
#                            df.write(" %.5f" % item)
#                            df.flush()
#                    df.write('\n')
#                    df.flush()
#
#
#
#                    df.write("#refine stage seed accuracy rate: ")
#                    for item in refine_rate_list:
#                            df.write(" %.5f" % item)
#                            df.flush()
#                    df.write('\n')
#                    df.flush()
#
#
#                    df.write("#z socre list: ");
#                    for item in z_score_list:
#                            df.write("%.5f " % item)
#                            df.flush()
#                    df.write('\n')
#                    df.write("#z cmty mapping accuracy rate: ")
#                    z_matched_cmty_index = [] 
#                    real_matched_cmty_index = []
#                    for i in range(len(z_score_list)):
#                            if z_score_list[i] > 2.0:
#                                    z_matched_cmty_index.append(i)
#                                    if(matched_index[i][3] > 0.3):
#                                            real_matched_cmty_index.append(i)
#                    tmp = float(len(real_matched_cmty_index))/len(z_matched_cmty_index)
#                    df.write(" %.5f" % tmp)
#                    df.write('\n')
#                    df.write("#total z community mapped number: %d\n"%(len(z_matched_cmty_index)))
#                    print "z community mapped accuracy rate: %f\n" % tmp
#                    df.write("#global seeds accuracy rate: %f\n" % global_seeds_accuracy_rate)
#                    df.write("#propagation_rate: %f\n" % refine_rate)
#                    df.write("########################################################################\n")
#            df.write("z score threshold end\n")
#
    print "mapping community finished!!"
    return 0
        

        
if __name__ == "__main__":
        main()

